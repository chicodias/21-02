---
title: "SME0809 - Inferência Bayesiana - Distribuição Normal"
author: "Grupo 13 - Francisco Miranda - 4402962 - Heitor Carvalho - 11833351"
date: "Outubro 2021"
output: pdf_document
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(dados)
#remotes::install_github("cienciadedatos/dados")

sample <- dados::pinguins|>
  filter(especie == "Pinguim-de-barbicha")|>
  select(comprimento_bico)|>
  drop_na()
```


# Caso 1: $\mu$ desconhecido e $\sigma$ conhecido

## Verossimilhança da distribuição

$$\mathcal{L}(y|\theta) =\prod_{i=1}^n p(y_i|\theta) =  \prod_{i-1}^n e^{-1/2\sigma^2(y_i - \theta)^2}$$

## A priori da distribuição

$$p(\theta)\propto e^{(-1/2\tau_0^2)(\theta-y_0)^2}$$
## A priori não informativa de Jeffreys

A distribuição a *priori* de Jeffreys é dada por $p(\theta) \propto \sqrt{J(n/\sigma^2)} \propto 1$

Sabemos que a Informação de Fisher de $\theta$ através de $y = y_1,...,y_n$ é definida como:

$$I(\theta) = E [-\frac{\partial^2log~p(y|\theta)}{\partial\theta^2}]$$
Segue que:

$$- E[\frac{\partial^2}{\partial\theta^2}(-log(2\pi\sigma^2)/2-1/2\sigma^2 (\sum_{i=1}^n(y_i-\theta)^2)] \\
= - E [\frac{\partial^2}{\partial\theta^2}(-log(2\pi\sigma^2)/2-1/2\sigma^2 (\sum_{i=1}^n(y_i^2-2\theta n\overline{y} + n\theta^2)] \\
= -E [\frac{\partial}{\partial\theta}\left(-1/2\sigma^2(-2n\overline{y} + 2n\theta\right))] \\
= -E[-1/2\sigma^2\left(2n)\right] \\
= n/\sigma^2$$

## A posteriori da distribuição

A posteriori é computada assumindo-se que:
1. Cada observação é independentemente distribuída
2. Cada observação tem a mesma variância

$$p(\theta|y) = p(\theta)~p(y|\theta) = \\ p(\theta)\prod_{i=1}^np(y_i|\theta) = e^{(-1/2\tau_0^2)(\theta-\mu_0)^2}\prod_{i=1}^ne^{(-1/2\sigma^2)(y_i-\theta)^2} =\\ e^{(\frac{-1}{2})(1/\tau_0^2(\theta-\mu_0)^2~ +\frac{1}{\sigma^2}\sum_{i=1}^n(y_i - \theta)^2 )}$$
Desse modo, a distribuição a posteriori da média $\theta$ depende apenas da média amostral $\overline{y} = \frac{1}{n}\sum_{i=1}^ny_i$, sendo assim, $\overline{y}$ é uma estatística suficiente.

Portanto, para $n$ observações, a posteriori apresenta a seguinte distribuição:

$$p(\theta|y_1,...,y_n) = p(\theta~|~\overline{y}) = \mathcal{N}(\theta|y_n, \tau_n^2)$$

Sendo, 

$$\mu_n = \frac{\tau_0^{-2} \mu_0 + n\sigma^{-2}\overline{y}}
{\tau_0^{-2} + \sigma^{-2}},\ \ \text{e}\ \ \tau_n^{-2}= \tau_0^{-2}+ n\sigma^{-2}$$
Podemos reescrever $p(\theta|y)$ como:

$$p(\theta|y_n) \propto e^{(-1/2\tau_n^2)(\theta-\mu_n)^2}$$

Logo, para uma distribuição Normal com variância conhecida, a média aposteriori $\mu_n$ pode ser interpretada como a média ponderada da média a priori e o valor observado $y = y_1,...,y_n$, sendo os pesos proporcionais às precisões de cada um.

```{r include=FALSE}
library(tidyverse)
library(dados)


sample <- dados::pinguins|>
  filter(especie == "Pinguim-de-barbicha")|>
  select(comprimento_bico)|>
  drop_na() |>
  pull()

```

```{r}
# gera a priori e a posteriori de uma normal com media desconhecida e sigma conhecido

 norm <- function(samp, sigma = 40, mu, tau0 = 10000 ){

  n <- length(samp)
  xbar <- mean(samp)

  ver <- function(x) exp(- n/(2*sigma^2) * (xbar - x)^2)

  mu.post <- (tau0^(-2)*mu + n*sigma^(-2)*xbar)/ (tau0^(-2) + n * sigma^(-2))
  sigma.post <- (tau0^(-2) + n*sigma^(-2))^(-1)

  theta <- seq(20, 60, 0.2)


  tibble(theta = theta,
         priori = dnorm(theta,mu,tau0),
         post = dnorm(theta,mu.post,sigma.post),
         ver = ver(theta),
         pred = dnorm(theta,mu.post,tau0+sigma),
         tau1 = sigma.post,
         mu1 = mu.post)

}
```


```{r}
norm(samp = sample,  sigma = sd(sample)+1, tau0 = 1e10, mu = 42) %>% 
  pivot_longer(cols = !"theta") %>% 
  ggplot(aes(x = theta)) +
  geom_line(aes(y = value)) +
  facet_wrap(~name, scales = "free_y")
```

```{r}
#coeff <- 0.03
a <- norm(samp = sample,  sigma = sd(sample)+1, tau0 = 20, mu = 42)

a|>  
  ggplot(aes(x = theta)) +
  geom_line(aes(y = post,  color = "Posteriori")) +
  geom_line(aes(y = priori, color = "Priori")) +
  geom_line(aes(y = ver, color = "Verossimilhança")) +
  scale_color_brewer(name = "Cor", type = "qual", palette = "Dark2")+
  scale_x_continuous(name = expression(theta), limits = c(20, 60))+
  #    theme(axis.title.y=element_blank()) +
  #scale_y_continuous(name = " ",
   #                  sec.axis = sec_axis(~.*1/coeff, name=expression(L(theta))))+
  ggtitle("Média Posteriori")+
  scale_linetype(name = "Distribuição")
```
```{r}
b <- norm(samp = sample,  sigma = sd(sample), tau0 = 8, mu = 30)

b|>  
  ggplot(aes(x = theta)) +
  geom_line(aes(y = post,  color = "Posteriori")) +
  geom_line(aes(y = priori, color = "Priori")) +
  geom_line(aes(y = ver, color = "Verossimilhança")) +
  scale_color_brewer(name = "Cor", type = "qual", palette = "Dark2")+
  scale_x_continuous(name = expression(theta), limits = c(20, 60))+
  #    theme(axis.title.y=element_blank()) +
  #scale_y_continuous(name = " ",
   #                  sec.axis = sec_axis(~.*1/coeff, name=expression(L(theta))))+
  ggtitle("Média Posteriori")+
  scale_linetype(name = "Distribuição")
```
```{r}
c <- norm(samp = sample,  sigma = sd(sample), tau0 = 14 , mu = 60)

c|>  
  ggplot(aes(x = theta)) +
  geom_line(aes(y = post,  color = "Posteriori")) +
  geom_line(aes(y = priori, color = "Priori")) +
  geom_line(aes(y = ver, color = "Verossimilhança")) +
  scale_color_brewer(name = "Cor", type = "qual", palette = "Dark2")+
  scale_x_continuous(name = expression(theta), limits = c(20, 60))+
  #    theme(axis.title.y=element_blank()) +
  #scale_y_continuous(name = " ",
   #                  sec.axis = sec_axis(~.*1/coeff, name=expression(L(theta))))+
  ggtitle("Média Posteriori")+
  scale_linetype(name = "Distribuição")
```
```{r}
d <- norm(samp = sample,  sigma = sd(sample), tau0 = 12, mu = 54)

d|>  
  ggplot(aes(x = theta)) +
  geom_line(aes(y = post,  color = "Posteriori")) +
  geom_line(aes(y = priori, color = "Priori")) +
  geom_line(aes(y = ver, color = "Verossimilhança")) +
  scale_color_brewer(name = "Cor", type = "qual", palette = "Dark2")+
  scale_x_continuous(name = expression(theta), limits = c(20, 60))+
  #    theme(axis.title.y=element_blank()) +
  #scale_y_continuous(name = " ",
   #                  sec.axis = sec_axis(~.*1/coeff, name=expression(L(theta))))+
  ggtitle("Média Posteriori")+
  scale_linetype(name = "Distribuição")
```

```{r}
tab <- tibble(      Prioris = c("Priori 1","Priori 2","Priori 3", "Priori 4"),
             Media.pri = c(42, 30, 60, 54),
             Media.pos = c(48.8, 48.8, 48.8, 48.8),
             SD.pri = c(2000, 1000, 800, 980),
             SD.pos = c(0.277, 0.164, 0.164, 0.164),
             IC.025 = c(qnorm(0.025, mean = 42 , sd = 48.8),
                        qnorm(0.025, mean = 30 , sd = 48.8),
                        qnorm(0.025, mean = 60, sd = 48.8),
                        qnorm(0.025, mean = 54, sd = 48.8)),
             IC.975 = c(qnorm(0.975, mean = 42 , sd = 48.8),
                        qnorm(0.975, mean = 30 , sd = 48.8),
                        qnorm(0.975, mean = 60, sd = 48.8),
                        qnorm(0.975, mean = 54, sd = 48.8)))|>
  knitr::kable()
```

Quatro amigos, Cleiton, Eduardo, Larissa e Robertinho resolvem tentar estimar a média do comprimento do bico dos pinguins de barbicha. Para tanto, cada integrante do grupo resolve dar um palpite em relação a média e variância da distribuição. Desse modo, Cleiton acredita que a distribuição seja próxima a $\mathcal {N}(42, 20)$, Eduardo acredita que $\mathcal {N}(30, 8)$, Larissa $\mathcal {N}(60, 14)$  e Robertinho $\mathcal {N}(54, 12)$.

Cleiton obteve uma $p(\theta|y) \sim \mathcal{N}(48.8, 0.277)$, Eduardo obteve uma$p(\theta|y) \sim \mathcal{N}(48.8, 0.164)$, Larissa obteve $p(\theta|y) \sim \mathcal{N}(48.8, 0.164)$ e Robertinho também obteve $p(\theta|y) \sim \mathcal{N}(48.8, 0.164)$ 

# Caso 2: $\mu$ conhecido e $\sigma$ desconhecido

## Distribuições a priori

Seja $Y_i$ uma amostra aleatória simples de uma distribuição $Y \sim N(\theta,\sigma^2)$, com $\theta$ conhecido.

Primeiramente, vamos encontrar a função de verossimilhança de $\sigma^2$.
  
$$\mathcal{L}(y|\sigma^2) = \prod_{i=1}^n \frac1{\sqrt{2\pi\sigma}} e^{-(y_i-\theta)^2/2\sigma^2} \propto
(\sigma^2)^{- \frac{n}{2}} e^{ -\left(\frac{1}{\sigma^{2}}\sum_{i=1}^n (y_i-\theta)^2 \right) }$$
  
### Priori não informativa
  
Definimos a log-verossimilhança em nosso caso como sendo:

$$\log(\mathcal{L}(y|\sigma^2)) \propto -\frac n2 \log(\sigma^{-2}) -\sigma^{-2}\sum_{i=1}^n (y_i-\theta)^2$$
  
A distribuição a *priori* de Jeffreys é dada por $\pi(\sigma^2) \propto \sqrt{J(\sigma^2)}$.

$$\begin{aligned}
J(\sigma^2) \propto E\left(- \frac{\partial^2}{\partial \theta^2}\log(L(\theta))\right) 
= E\left(- \frac{\partial^2}{\partial \theta^2}\left(-\frac n2 \log(\sigma^{-2}) -(\sigma^{2})^{-1}\sum_{i=1}^n (y_i-\theta)^2\right)\right)\\
= E\left(- \frac{\partial}{\partial \theta}\left(-\frac n2 (\sigma^{2})^{-1} + (\sigma^{2})^{-2}\sum_{i=1}^n (y_i-\theta)^2\right)\right) 
= E\left(- \frac{n}{2\sigma^2} + 2(\sigma^{2})^{-3}\sum_{i=1}^n (y_i-\theta)^2\right) \\
= - \frac{n}{2\sigma^2} + 2\sigma^{-4}\sum_{i=1}^n (E(y_i)-\theta)^2 
= - \frac{n}{\sigma^2} + 2\sigma^{-4}\sum_{i=1}^n (\theta-\theta)^2
= - \frac{n}{\sigma^2} \propto {\sigma^{-2}}
\end{aligned}$$

Assim, $\pi(\sigma) \propto \sqrt{\sigma^{-2}} = \sigma^{-1}$. Seu parâmetro $\Phi$ de escala que faz com que $\theta$ mude somente em locação pode ser obtido através do cálculo de

$$\phi \propto \int \pi(\sigma^2) d\sigma^2 = \int \frac1{\sigma^{2}} d\sigma^2 = \log|\sigma^2| + k \propto
\log\sigma^2 $$

$\phi$ é uma distribuição imprópria, pois $\int_0^{+\infty} \log(\sigma^2) d\sigma^2$ é divergente. Assim, a *priori* não favorece nenhuma escala em detrimento de outra.
  
### Conjulgadas Naturais
  
  O suporte de nosso parâmetro de interesse $\sigma >0$ permite-nos adotar três distribuições de probabilidade estudadas durante o curso:
  
  1.**Gama:** 
  
  Se $X \sim \text{Gama}(\alpha,\beta)$ então
$$f_{X}(x | \alpha,\beta) = \frac{\beta^\alpha x^{(\alpha - 1)} e^{-\beta x}}{\Gamma(\alpha)} \propto x^{(\alpha - 1)} e^{-\beta x}, \ \ \alpha>0, \beta >0, x>0$$

  
  2.**Gama-Inversa:** 
  
  Se $X \sim \text{Gama-Inv}(\alpha,\beta)$ então
$$ f_{X}(x | \alpha,\beta) = \frac{\beta^\alpha x^{-(\alpha + 1)} e^{-\beta / x}}{\Gamma(\alpha)} \propto x^{-(\alpha + 1)} e^{-\beta/x}, \ \ \alpha>0, \beta >0, x>0$$
  
  3.**Qui-Quadrado:** 

  Se $X\sim \chi^2 (\nu)$ então
$$ f_{X}(x | \nu) = \frac{x^{(\nu/2) - 1} e^{- x/2}}{2^{\nu/2}\Gamma(\nu/2)} \propto x^{(\nu/2) - 1} e^{- x/2}, \ \ \nu>0, x>0$$  

Note que as duas primeiras estão relacionadas via uma transformação simples e a última é um caso particular delas. Desprezadas as constantes não informativas, as três distribuições são da forma $x$ elevado a uma potência vezes a exponencial de $x$. Dessa forma, as três distribuições servem como conjulgada natural da Normal, em nosso caso. Neste trabalho, optou-se por utilizar a distribuição Gama Inversa.

Fazendo $x = \sigma$, temos uma *priori* da forma:
  
$$\sigma^{-(\alpha + 1)} e^{-\beta / \sigma^2} \Rightarrow \pi(\sigma) \sim \text{Gama-Inv}(\alpha,\beta)$$
Se quisermos torná-la não informativa, basta utilizarmos $\alpha \to 0, \beta \to 0$.


## Distribuição a *posteriori*

$$\pi(\sigma|y) \propto \mathcal{L}(y|\sigma^2) \pi(\sigma) =
(\sigma^2)^{- \frac{n}{2}} e^{ -\left(\frac{1}{2\sigma^{2}}\sum_{i=1}^n (y_i-\theta)^2 \right) } \sigma^{-(\alpha - 1)} e^{-\beta/ \sigma^2}$$

$$= (\sigma^2)^{-(\alpha + \frac{n}{2} + 1)} e^{ -\frac{1}{\sigma^2}\left(\beta +\frac{1}{2}\sum_{i=1}^n (y_i-\theta)^2 \right)}$$

Dessa forma,

$$\pi(\sigma^2 | y) \sim \text{Gama-Inv}(\alpha + \frac n2,\beta +\frac{1}{2}\sum_{i=1}^n (y_i-\theta)^2)$$


```{r}
library(tidyverse)
library(effectsize)
library(invgamma)
library(dados)


sample <- dados::pinguins %>%
  filter(especie == "Pinguim-de-barbicha") %>%
  select(comprimento_bico) %>% 
  drop_na() %>% pull()

#sample <- rnorm(10, 5, 4)



```


```{r include=FALSE}


# gera a priori e a posteriori de uma normal com media conhecida e sigma desconhecido

SigmaNorm <- function(samp, theta = 5, alpha = 0.001, beta = 0.001){
  
  n <- length(samp)

  s <- sum(((samp - theta)/2)^2)
  
  l_sigma2 <- function(sigma2) sigma2^(-(n/2))  * exp(- 1/sigma2 *s)

  
  a.post <- alpha + n/2
  b.post <- beta +  s
  
  sigma2 <- seq(0.02, 40, 0.02)
  
  tibble(sigma2 = sigma2,
         priori = normalize( dinvgamma(sigma2,alpha,beta)),
         post = normalize(dinvgamma(sigma2,a.post,b.post)),
         ver = normalize(l_sigma2(sigma2)),
         alpha1 = a.post,
         beta1 = b.post)
}
```


```{r}

SigmaNorm(sample, mean(sample))[-c(5,6)] %>% 
  pivot_longer(cols = !"sigma2") %>% 
  ggplot(aes(x = sigma2)) +
  geom_line(aes(y = value)) +
  facet_wrap(~name, scales = "free_y")
```

```{r}

SigmaNorm(sample, 48.83382 ,alpha = 34, beta = 187)[-c(5,6)] %>% 
  pivot_longer(cols = !"sigma2") %>% 
  ggplot(aes(x = sigma2)) +
  geom_line(aes(y = value)) +
  facet_wrap(~name, scales = "free_y")
```
```{r}

SigmaNorm(sample, mean(sample))[-c(5,6)] %>% 
  pivot_longer(cols = !"sigma2") %>% 
  ggplot(aes(x = log(sigma2))) +
  geom_line(aes(y = log(value))) +
  facet_wrap(~name, scales = "free_y")
```

