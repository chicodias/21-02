---
title: "SME0809 - Inferência Bayesiana - Distribuição Normal"
author: "Grupo 13 - Francisco Miranda - 4402962 - Heitor Carvalho - 11833351"
date: "Outubro 2021"
output: pdf_document
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dados)
#remotes::install_github("cienciadedatos/dados")

sample <- dados::pinguins|>
  filter(especie == "Pinguim-de-barbicha")|>
  select(comprimento_bico)|>
  drop_na()
```


# Caso 1: $\mu$ desconhecido e $\sigma$ conhecido

## Verossimilhança da distribuição

$$\mathcal{L}(y|\theta) =\prod_{i=1}^n p(y_i|\theta) =  \prod_{i-1}^n e^{-1/2\sigma^2(y_i - \theta)^2}$$

## A priori da distribuição

$$p(\theta)\propto e^{(-1/2\tau_0^2)(\theta-y_0)^2}$$
## A priori não informativa de Jeffreys

A distribuição a *priori* de Jeffreys é dada por $p(\theta) \propto \sqrt{J(n/\sigma^2)} \propto 1$

Sabemos que a Informação de Fisher de $\theta$ através de $y = y_1,...,y_n$ é definida como:

$$I(\theta) = E [-\frac{\partial^2log~p(y|\theta)}{\partial\theta^2}]$$
Segue que:

$$- E[\frac{\partial^2}{\partial\theta^2}(-log(2\pi\sigma^2)/2-1/2\sigma^2 (\sum_{i=1}^n(y_i-\theta)^2)] \\
= - E [\frac{\partial^2}{\partial\theta^2}(-log(2\pi\sigma^2)/2-1/2\sigma^2 (\sum_{i=1}^n(y_i^2-2\theta n\overline{y} + n\theta^2)] \\
= -E [\frac{\partial}{\partial\theta}\left(-1/2\sigma^2(-2n\overline{y} + 2n\theta\right))] \\
= -E[-1/2\sigma^2\left(2n)\right] \\
= n/\sigma^2$$

## A posteriori da distribuição

A posteriori é computada assumindo-se que:
1. Cada observação é independentemente distribuída
2. Cada observação tem a mesma variância

$$p(\theta|y) = p(\theta)~p(y|\theta) = \\ p(\theta)\prod_{i=1}^np(y_i|\theta) = e^{(-1/2\tau_0^2)(\theta-\mu_0)^2}\prod_{i=1}^ne^{(-1/2\sigma^2)(y_i-\theta)^2} =\\ e^{(\frac{-1}{2})(1/\tau_0^2(\theta-\mu_0)^2~ +\frac{1}{\sigma^2}\sum_{i=1}^n(y_i - \theta)^2 )}$$
Desse modo, a distribuição a posteriori da média $\theta$ depende apenas da média amostral $\overline{y} = \frac{1}{n}\sum_{i=1}^ny_i$, sendo assim, $\overline{y}$ é uma estatística suficiente.

Portanto, para $n$ observações, a posteriori apresenta a seguinte distribuição:

$$p(\theta|y_1,...,y_n) = p(\theta~|~\overline{y}) = \mathcal{N}(\theta|y_n, \tau_n^2)$$

Sendo, 

$$\mu_n = \frac{\tau_0^{-2} \mu_0 + n\sigma^{-2}\overline{y}}
{\tau_0^{-2} + \sigma^{-2}},\ \ \text{e}\ \ \tau_n^{-2}= \tau_0^{-2}+ n\sigma^{-2}$$
Podemos reescrever $p(\theta|y)$ como:

$$p(\theta|y_n) \propto e^{(-1/2\tau_n^2)(\theta-\mu_n)^2}$$

Logo, para uma distribuição Normal com variância conhecida, a média aposteriori $\mu_n$ pode ser interpretada como a média ponderada da média a priori e o valor observado $y = y_1,...,y_n$, sendo os pesos proporcionais às precisões de cada um.

```{r include=FALSE}
library(tidyverse)
library(dados)


sample <- dados::pinguins|>
  filter(especie == "Pinguim-de-barbicha")|>
  select(comprimento_bico)|>
  drop_na() |>
  pull()

# gera a priori e a posteriori de uma normal com media desconhecida e sigma conhecido

#Media a priori
MediaNorm <- function(samp,xbar=1, sigma = 3.3){
  
  n <- length(samp)
  
  loglik <- function(theta) exp((-1/2*sigma^2)*(n*theta-sum(samp))^2)
  
  tau_n <- tau_0^-2
  mu.post <- (tau0^(-2)*mu + n*sigma^(-2)*xbar)/ (tau0^(-2) + n * sigma^(-2))
  sigma.post <- (tau0^(-2) + n*sigma^(-2))^(-1)
  
  theta <- seq(mu - 10* sigma, mu + 10 * sigma,0.5)
  
  
  tibble(theta = theta,
         priori = dnorm(theta,xbar, sigma),
         post = dnorm(theta,mu.post,sigma.post),
         ver = loglik(theta),
         pred = dnorm(theta,mu.post,tau0+sigma),
         tau1 = sigma.post,
         mu1 = mu.post)
  
}

```

```{r}
MediaNorm(sample)
```


```{r include=FALSE}
# library(tidyverse)
# 
# dados <- c(921.3429, 910.8055, 879.7302, 899.3456, 837.7587, 880.95, 830.4974, 856.1041, 777.5677, 983.3444, 981.048, 863.2836, 943.1702, 904.5064, 831.879, 854.2947, 920.2139, 839.7896, 897.0014, 839.3796, 872.4004, 884.6379, 892.7966, 897.9938, 840.228, 823.2803, 903.4257, 798.5764, 862.5581, 938.8837, 894.951, 871.0477, 863.9725, 857.6077, 856.3452, 906.6818, 859.7368, 876.7236, 868.8648, 900.386, 839.8117, 878.5936, 866.6935, 921.2941, 877.8528, 897.6217, 894.6499, 830.6403, 935.6717, 948.9588, 888.0726, 788.6694, 877.4076, 863.1756, 982.6952, 934.724, 898.8038, 874.8416, 905.8379, 843.0447, 877.0727, 894.5569, 959.8427, 826.6365, 907.9904, 912.9051, 912.4662, 975.3224, 922.8291, 934.6569, 953.236, 833.5084, 842.0623, 885.3881, 918.0987, 842.0927, 875.6968, 826.3312, 870.4634, 866.2226, 845.4767, 914.2475, 922.4006, 832.359, 877.5627, 887.8087, 879.4061, 925.3103, 877.6632, 877.3689, 854.1189, 904.5891, 882.893, 866.3494, 892.4554, 908.3249, 873.6439, 914.1283, 879.8893, 843.1085)
# 
# 
# # gera a priori e a posteriori de uma normal com media desconhecida e sigma conhecido
# 
# norm <- function(samp, sigma = 40, mu = 900, tau0 = 20){
#   
#   n <- length(samp)
#   xbar <- mean(samp)
#   
#   ver <- function(x) exp(- n/(2*sigma^2) * (xbar - x)^2)
#   
#   
#   mu.post <- (tau0^(-2)*mu + n*sigma^(-2)*xbar)/ (tau0^(-2) + n * sigma^(-2))
#   sigma.post <- (tau0^(-2) + n*sigma^(-2))^(-1)
#   
#   theta <- seq(mu - 10* sigma, mu + 10 * sigma,0.5)
#   
#   
#   tibble(theta = theta,
#          priori = dnorm(theta,mu,tau0),
#          post = dnorm(theta,mu.post,sigma.post),
#          ver = ver(theta),
#          pred = dnorm(theta,mu.post,tau0+sigma),
#          tau1 = sigma.post,
#          mu1 = mu.post)
#   
# }


```


$$f(x|\theta) = e^{-x^2}$$
  
  
  - a) Faça um esboço do gráfico das distribuições prioris dos dois físicos em um mesmo sistema cartesiano.


Temos $\theta_A \sim N(900, 20^2)$ e $\theta_B \sim N(900, 80^2)$. Assim:
  
```{r echo = FALSE, warning = FALSE}
<<<<<<< HEAD
# 
# a <- rbind(norm(dados, tau0 = 20)|> mutate(cor = "Fisico A", obs = "100 observações"),
#            norm(dados, tau0 = 80)|> mutate(cor = "Fisico B", obs = "100 observações"),
#            norm(892, tau0 = 20)|> mutate(cor = "Fisico A", obs = "1 observação"),
#            norm(892, tau0 = 80)|> mutate(cor = "Fisico B", obs = "1 observação"))
# 
# a|> filter(obs == "100 observações")|>
#   ggplot(aes(x = theta)) +
#   geom_line(aes(y = priori, color = cor)) +
#   scale_colour_brewer(name = "Distribuição", type = "qual", palette = "Dark2")+
#   scale_x_continuous(name = expression(theta), limits = c(700, 1100))+
#   theme(axis.title.y=element_blank()) +
#   ggtitle("Distribuição a priori da grandeza estimada pelos físicos")
=======

a <- rbind(norm(dados, tau0 = 20)|> mutate(cor = "Fisico A", obs = "100 observações"),
           norm(dados, tau0 = 80)|> mutate(cor = "Fisico B", obs = "100 observações"),
           norm(892, tau0 = 20)|> mutate(cor = "Fisico A", obs = "1 observação"),
           norm(892, tau0 = 80)|> mutate(cor = "Fisico B", obs = "1 observação"))

a|> filter(obs == "100 observações")|>
  ggplot(aes(x = theta)) +
  geom_line(aes(y = priori, color = cor)) +
  scale_colour_brewer(name = "Distribuição", type = "qual", palette = "Dark2")+
  scale_x_continuous(name = expression(theta), limits = c(700, 1100))+
  theme(axis.title.y=element_blank()) +
  ggtitle("Distribuição a priori da grandeza estimada pelos físicos")
>>>>>>> 28667acb8a665ea4764d398640aafb73bc34d1b2

```

- b) Encontre a distribuição a posteriori para o físico A e para o físico B.

Como $X|\theta \sim N(\theta, \sigma^2)$ com $\sigma^2$ conhecido e $\theta \sim N(\mu_0,\tau_0^2)$ então $\theta|x \sim N(\mu_1, \tau_1)$, sendo

$$\mu_1 = \frac{\tau_0^2 \mu_0 + \sigma^{-2}x}{\tau_0^{-2} + \sigma^{-2}},\ \ \text{e}\ \ \tau_1^{-2}= \tau_0^{-2}+ \sigma^{-2}$$
  
  Assim, para 100 observações temos:
  
  $$\theta_A \sim N(884.314, 15.3846), \ \ \theta_B \sim N(883.7272, 15.9601) $$
  
  Enquanto que para uma única observação, a posteriori é:
  
  $$\theta_A \sim N(898.4, 320), \ \ \theta_B \sim N(893.6, 1280) $$
  
  
```{r echo = FALSE, warning = FALSE}
# a|>
#   ggplot(aes(x = theta)) +
#   geom_line(aes(y = post, color = cor)) +
#   #geom_line(aes(y = 0.03* ver, colour = "Verossimilhança")) +
#   scale_colour_brewer(name = "Distribuição", type = "qual", palette = "Dark2")+
#   scale_x_continuous(name = expression(theta), limits = c(700, 1100))+
#   theme(axis.title.y=element_blank()) +
#   ggtitle("Distribuição a posteriori da grandeza estimada pelos físicos") +
#   facet_wrap(~obs)

```


- c) Faça um esboço do gráfico das distribuições: a priori e a posteriori de cada um dos dois físicos em um mesmo sistema cartesiano.

```{r echo = FALSE, warning = FALSE}
# a|>
#   ggplot(aes(x = theta)) +
#   geom_line(aes(y = post, color = "Posteriori")) +
#   geom_line(aes(y = priori, color = "Priori")) +
#   #geom_line(aes(y = pred, color = "Preditiva")) +
#   #geom_line(aes(y = 0.03* ver, colour = "Verossimilhança")) +
#   scale_colour_brewer(name = "Distribuição", type = "qual", palette = "Dark2")+
#   scale_x_continuous(name = expression(theta), limits = c(700, 1100))+
#   theme(axis.title.y=element_blank()) +
#   ggtitle("Distribuição da grandeza estimada pelos físicos")+
#   facet_wrap(cor~obs)

```


```{r echo = FALSE, warning = FALSE}
# coeff <- 0.03
# a|>
#   ggplot(aes(x = theta)) +
#   geom_line(aes(y = post, color = cor, linetype = "Posteriori")) +
#   geom_line(aes(y = priori, color = cor, linetype = "Priori")) +
#   geom_line(aes(y = coeff* ver, colour = "Verossimilhança")) +
#   scale_colour_brewer(name = "Cor", type = "qual", palette = "Dark2")+
#   scale_x_continuous(name = expression(theta), limits = c(700, 1100))+
#   #    theme(axis.title.y=element_blank()) +
#   scale_y_continuous(name = " ",
#                      sec.axis = sec_axis(~.*1/coeff, name=expression(L(theta))))+
#   ggtitle("Distribuição da grandeza estimada pelos físicos")+
#   scale_linetype(name = "Distribuição") +
#   facet_wrap(~obs)

```


- d) Observando o gráfico, qual físico aprendeu mais com o experimento? Justifique.

Aumentos nas precisões a posteriori em relação às precisões a priori com 100 observações:
  
  - para o físico A: precisão($\theta$) passou de $\tau_0^{-2} = 0.0025$ a $\tau_1^{-2} = 0.004225008$ (aumento de 70%).
- para o físico B: precisão($\theta$) passou de $\tau_0^{-2} = 0.00015625$ a $\tau_1^{-2} = 0.003926$ (aumento de 2500%)

Com 1 observação:
  
  - para o físico A: precisão($\theta$) passou de $\tau_0^{-2} = 0.0025$ a $\tau_1^{-2} = 6.1035*10^{-7}$.
- para o físico B: precisão($\theta$) passou de $\tau_0^{-2} = 0.00015625$ a $\tau_1^{-2} =  9.7656 *10^{-6}$.


- e) Construa uma tabela que contenha o resumo a priori e o resumo a posteriori.

```{r echo = FALSE, warning = FALSE}

# tibble(      Fisico = c("A","B"),
#              Media.pri = c(900, 900),
#              Media.pos.100 = c(884.314,883.7272),
#              Media.pos.1 = c(898.4 ,893.6),
#              SD.pri = c(20, 80),
#              SD.pos.100 = c(15.3846, 15.9601),
#              SD.pos.1 = c(320 ,1280))|> 
#   knitr::kable()

```





- f) Encontre a distribuição preditiva e faça um esboço de seu gráfico.

A distribuição preditiva é dada por:
  
  $$X \sim N(\mu_0, \tau_0^2 + \sigma^2) $$
  
```{r echo = FALSE, warning = FALSE}
# a|> filter(obs == "100 observações")|> 
#   ggplot(aes(x = theta)) +
#   geom_line(aes(y = pred, color = cor, linetype = "Preditiva")) +
#   geom_line(aes(y = priori, color = cor, linetype = "Priori")) +
#   scale_colour_brewer(name = "Cor", type = "qual", palette = "Dark2")+
#   scale_x_continuous(name = expression(theta), limits = c(700, 1100))+
#   theme(axis.title.y=element_blank()) +
#   ggtitle("Distribuição da grandeza estimada pelos físicos")+
#   scale_linetype(name = "Distribuição")

```


# Caso 2: $\mu$ conhecido e $\sigma$ desconhecido

## Distribuições a priori

Seja $Y_i$ uma amostra aleatória simples de uma distribuição $Y \sim N(\theta,\sigma^2)$, com $\theta$ conhecido.

Primeiramente, vamos encontrar a função de verossimilhança de $\sigma^2$.
  
$$\mathcal{L}(y|\sigma^2) = \prod_{i=1}^n \frac1{\sqrt{2\pi\sigma}} e^{-(y_i-\theta)^2/2\sigma^2} \propto
(\sigma^2)^{- \frac{n}{2}} e^{ -\left(\frac{1}{\sigma^{2}}\sum_{i=1}^n (y_i-\theta)^2 \right) }$$
  
### Priori não informativa
  
  
$$\log(\mathcal{L}(y|\sigma^2)) \propto -\frac n2 \log(\sigma^{-2}) -\sigma^{-2}\sum_{i=1}^n (y_i-\theta)^2$$
  
A distribuição a *priori* de Jeffreys é dada por $\pi(\sigma^2) \propto \sqrt{J(\sigma^2)}$.

$$\begin{aligned}
J(\sigma^2) \propto E\left(- \frac{\partial^2}{\partial \theta^2}\log(L(\theta))\right) 
= E\left(- \frac{\partial^2}{\partial \theta^2}\left(-\frac n2 \log(\sigma^{-2}) -(\sigma^{2})^{-1}\sum_{i=1}^n (y_i-\theta)^2\right)\right)\\
= E\left(- \frac{\partial}{\partial \theta}\left(-\frac n2 (\sigma^{2})^{-1} + (\sigma^{2})^{-2}\sum_{i=1}^n (y_i-\theta)^2\right)\right) 
= E\left(- \frac{n}{2\sigma^2} + 2(\sigma^{2})^{-3}\sum_{i=1}^n (y_i-\theta)^2\right) \\
= - \frac{n}{2\sigma^2} + 2\sigma^{-4}\sum_{i=1}^n (E(y_i)-\theta)^2 
= - \frac{n}{\sigma^2} + 2\sigma^{-4}\sum_{i=1}^n (\theta-\theta)^2
= - \frac{n}{\sigma^2} \propto {\sigma^{-2}}
\end{aligned}$$

Assim, $\pi(\sigma) \propto \sqrt{\sigma^{-2}} = \sigma^{-1}$. Seu parâmetro $\Phi$ de escala que faz com que $\theta$ mude somente em locação pode ser obtido através do cálculo de

$$\phi \propto \int \pi(\sigma^2) d\sigma^2 = \int \frac1{\sigma^{2}} d\sigma^2 = \log|\sigma^2| + k \propto
\log\sigma^2 $$

$\phi$ é uma distribuição imprópria, pois $\int_0^{+\infty} \log(\sigma^2) d\sigma^2$ é divergente. Assim, a *priori* não favorece nenhuma escala em detrimento de outra.
  
### Conjulgadas Naturais
  
  O suporte de nosso parâmetro de interesse $\sigma >0$ permite-nos adotar três distribuições de probabilidade estudadas durante o curso:
  
  1.**Gama:** 
  
  Se $X \sim \text{Gama}(\alpha,\beta)$ então
$$f_{X}(x | \alpha,\beta) = \frac{\beta^\alpha x^{(\alpha - 1)} e^{-\beta x}}{\Gamma(\alpha)} \propto x^{(\alpha - 1)} e^{-\beta x}, \ \ \alpha>0, \beta >0, x>0$$

  
  2.**Gama-Inversa:** 
  
  Se $X \sim \text{Gama-Inv}(\alpha,\beta)$ então
$$ f_{X}(x | \alpha,\beta) = \frac{\beta^\alpha x^{-(\alpha + 1)} e^{-\beta / x}}{\Gamma(\alpha)} \propto x^{-(\alpha + 1)} e^{-\beta/x}, \ \ \alpha>0, \beta >0, x>0$$
  
  3.**Qui-Quadrado:** 

  Se $X\sim \chi^2 (\nu)$ então
$$ f_{X}(x | \nu) = \frac{x^{(\nu/2) - 1} e^{- x/2}}{2^{\nu/2}\Gamma(\nu/2)} \propto x^{(\nu/2) - 1} e^{- x/2}, \ \ \nu>0, x>0$$  

Note que as duas primeiras estão relacionadas via uma transformação simples e a última é um caso particular delas. Desprezadas as constantes não informativas, as três distribuições são da forma $x$ elevado a uma potência vezes a exponencial de $x$. Dessa forma, as três distribuições servem como conjulgada natural da Normal, em nosso caso. Neste trabalho, optou-se por utilizar a distribuição Gama Inversa.

Fazendo $x = \sigma$, temos uma *priori* da forma:
  
$$\sigma^{-(\alpha + 1)} e^{-\beta / \sigma^2} \Rightarrow \pi(\sigma) \sim \text{Gama-Inv}(\alpha,\beta)$$
Se quisermos torná-la não informativa, basta utilizarmos $\alpha \to 0, \beta \to 0$.


## Distribuição a *posteriori*

$$\pi(\sigma|y) \propto \mathcal{L}(y|\sigma^2) \pi(\sigma) =
(\sigma^2)^{- \frac{n}{2}} e^{ -\left(\frac{1}{2\sigma^{2}}\sum_{i=1}^n (y_i-\theta)^2 \right) } \sigma^{-(\alpha - 1)} e^{-\beta/ \sigma^2}$$

$$= (\sigma^2)^{-(\alpha + \frac{n}{2} + 1)} e^{ -\frac{1}{\sigma^2}\left(\beta +\frac{1}{2}\sum_{i=1}^n (y_i-\theta)^2 \right)}$$

Dessa forma,

$$\pi(\sigma^2 | y) \sim \text{Gama-Inv}(\alpha + \frac n2,\beta +\frac{1}{2}\sum_{i=1}^n (y_i-\theta)^2)$$




```{r include=FALSE}
library(tidyverse)
library(dados)

sample <- dados::pinguins|>
  filter(especie == "Pinguim-de-barbicha")|>
  select(comprimento_bico)|> 
  drop_na()

# gera a priori e a posteriori de uma normal com media conhecida e sigma desconhecido

SigmaNorm <- function(samp, theta = 49, alpha = 0.001, beta = 0.001){
  
  n <- length(samp)
  xbar <- mean(samp)
  
  loglik <- function(sigma2) - (n/2) * exp(sigma^2) - 1/sigma2 * sum(samp - theta)^2
  
  
  mu.post <- (tau0^(-2)*mu + n*sigma^(-2)*xbar)/ (tau0^(-2) + n * sigma^(-2))
  sigma.post <- (tau0^(-2) + n*sigma^(-2))^(-1)
  
  theta <- seq(mu - 10* sigma, mu + 10 * sigma,0.5)
  
  
  tibble(theta = theta,
         priori = dgamma(sigma2,alpha,beta),
         post = dnorm(theta,mu.post,sigma.post),
         ver = ver(theta),
         pred = dnorm(theta,mu.post,tau0+sigma),
         tau1 = sigma.post,
         mu1 = mu.post)
  
}


```
