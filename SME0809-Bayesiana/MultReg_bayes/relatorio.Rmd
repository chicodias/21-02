---
title: "SME0809 - Inferência Bayesiana - Prova 2 - Grupo 13"
subtitle: "High-Dimensional Multivariate Bayesian Variable and Covariance Selection in Linear Regression [@BayesSUR2021]"
author:
  - "Francisco Miranda - 4402962"
  - "Heitor Carvalho - 11833351"
date: "Dezembro 2021"
institution: "ICMC - USP"
output: pdf_document
bibliography: packages.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introdução 

Com o desenvolvimento de técnicas de alto processamento na biologia molecular, a caracterização molecular em alta escala tornou-se um lugar comum, com o advento de técnicas como:

 - genome-wide measurement of gene expression
 - single nucleotide polymorphisms
 - CpG methylation status
 - pharmacological profiling for large-scale cancer drug screen.
 
A análise de associações conjuntas entre múltiplos fenótipos correlacionados e atributos moleculares de alta dimensionalidade é desafiadora.


Quando múltiplos fenótipos e informação genômica de alta dimensionalidade são analisados conjuntamente, a abordagem bayesiana permite especificar de maneira flexível as relações complexas entre os conjunto de dados altamente estruturados.

O pacote `BayesSUR` combina diversos modelos que foram propostos para a regressão multidimentional com resposta múltipla e introduz um novo modelo, que permite diferentes *prioris* na seleção de variáveis dos modelos de regressão e para diferentes pressupostos a respeito da estrutura de dependência entre as respostas.


# Metodologia

 - múltiplas opções de seleção de variáveis
 - a matriz de covariância pode ser diagonal, densa ou esparsa.
 - engloba três classes de modelos de regressão linear de múltipla resposta:
  - HRR
  - dSUR e SSUR
  - MRF
 
 
 O modelo de regressão é escrito como:
 
\begin{equation}
\boldsymbol{Y} = \boldsymbol{XB} + \boldsymbol{U} \label{eq:1}
\end{equation}

$$\text{vec}{(\boldsymbol{U})} \sim \mathcal{N}(\boldsymbol{0}, C \otimes \mathbb{I}_n) $$

onde:

 - $\boldsymbol{Y}$ é uma matriz $s \times s$ das variáveis resposta com matriz de covariância C;
 - $\boldsymbol{X}$ é uma matriz $n \times p$ de preditores para todas as respostas;
 - $\boldsymbol{U}$ é a matriz dos resíduos;
 - $\text{vec}(\cdot)$ denota a vetorização da matriz;
 - $\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$ denota uma distribuição normal multivariada com vetor de médias $\boldsymbol{\mu}$ e matriz de covariâncias $\boldsymbol{\Sigma}$;
 - $\boldsymbol{0}$ denota um vetor coluna com todos os elementos nulos,
 - $\otimes$ é o produto de Kronecker e $\mathbb{I}_n$ a matriz identidade de ordem $n$.


A seleção de variáveis é realizada através de uma matriz indicadora binária latente $\boldsymbol{\Gamma} = \{\gamma_{jk}\}$.

Uma *priori* "spike-and-slab" é utilizada para encontrar um subconjunto esparso relevante de preditores que expliquem a variabilidade de $\boldsymbol{Y}$: condicional em $\gamma_{jk} = 0\ \ (j = 1, ..., p, \text{e}\ k=1, ..., s)$

Definem-se $\beta_{jk} = 0$ condicionado em $\gamma_{jk} = 1$ seguem uma distribuição normal difusa:

\begin{equation}
\beta_\gamma \sim \mathcal{N}(\textbf{0}, W^{-1}_\gamma) \label{eq:2}
\end{equation}

Onde $\beta = \text{vec}(\textbf{B})$, $\gamma = \text{vec}( \boldsymbol{\Gamma})$, $\beta_\gamma$ consiste somente nos coeficientes selecionados (i.e. $\gamma_{jk} = 1$), assim $W_\gamma$ é a sub matriz de W formada pelos coeficientes selecionados correspondentes.

A matriz de precisão, W, é geralmente decomposta em coeficientes de encolhimento e uma matriz que governa a estrutura de covariância dos coeficientes de regressão. É utilizado aqui $W = w^{-1} \mathbb{I}_{sp}$, o que significa que todos os coeficientes de regressão são independentes a priori, com uma *hiperpriori* no coeficiente de encolhimento $w$, i.e. $w \sim \mathcal{IG}\text{amma}(a_w, b_w)$.

A matriz indicadora binária latente $\boldsymbol \Gamma$ tem três opções de *priori*:

  - Bernoulli independente hierárquica
  - hotspot prior
  - MRF prior
  
A matriz de covariância $C$ também possui três *prioris*:

  - Gama inversa independente
  - Wishart inversa
  - hiper-inversa Wishart
  
São considerados no total nove possíveis modelos dentre as combinações de $C$ e $\boldsymbol \Gamma$

|  | ${\gamma_{jk}} \sim Bernoulli$ | $\gamma_{jk} \sim$ hotspot | $\gamma_{jk} \sim$ MRF |
|:---:|:---:|:---:|:---:|
| $C\sim indep$ | HRR-B | HRR-H | HRR-M |
| $C\sim IW$ | dSUR-B | dSUR-H | dSUR-M |
| $C\sim HIW$ | SSUR-B | SSUR-H | SSUR-M |

##  Regressão Hierárquica Relacionada (HRR)

A Regressão Hierárquica Relacionada assume que $C$ é uma matriz diagonal, o que se traduz em independência condicional entre múltiplas variáveis resposta.

Uma *priori* gama inversa é especificada para a covariância dos resíduos, i.e

$$\sigma^2_k \sim \mathcal{IG}\text{amma}(a_\sigma, b_\sigma)$$

Quando combinada com as *prioris* em \eqref{eq:2}, é conjulgado com a verossimilhança do modelo \eqref{eq:1}. Podemos então amostrar a estrutura de seleção de variáveis $\boldsymbol{\Gamma}$ marginalmente com respeito a $C$ e $\boldsymbol B$.

### HRR com uma *priori* Bernouli independente

Para uma *priori* simples de seleção do modelo de regressão, os indicadores binários latentes seguem uma *priori* de Bernoulli:

\begin{equation}
\gamma_{jk}|\omega_{jk} \sim \mathcal Ber(\omega_{jk})\ \ (j = 1, ..., p, \text{e}\ k=1, ..., s)  \label{eq:3} 
\end{equation}


Com uma priori hierárquica Beta em $\omega_j$, i.e. $\omega_j \sim \mathcal Beta(a_\omega, b_\omega)$, que quantifica a probabilidade de cada preditor ser associado com qualquer uma das variáveis resposta.

### HRR com uma *priori* hotspot

É proposta a decomposição da probabilidade do parâmetro de associação $\omega_{jk}$ em \eqref{eq:3}, onde $o_k$ é responsável pela esparsividade de cada modelo de resposta e $\pi_j$ controla a propensão de cada preditor a ser associado a múltiplas respostas simuntaneamente:

\begin{equation}
\gamma_{jk}|\omega_{jk} \sim \mathcal Ber(\omega_{jk})\ \ (j = 1, ..., p, \text{e}\ k=1, ..., s)  \label{eq:4} 
\end{equation}
$$\begin{gathered}
\omega_{jk} = o_k \times \pi_j \\
o_k \sim \mathcal Beta (a_0, b_0) \\
\pi_j \sim \mathcal Gamma(a_\pi, b_\pi)
\end{gathered}$$

## Regressão não relacionada aparentemente esparsa (SSUR)

Para modelar a matriz de covariância $C$ é especificado uma *priori* hiper-Inversa Wishart, o que significa que as variáveis resposta têm por trás um grafo $\mathcal G$ que codifica a dependência condicional entre as respostas.

Um grafo esparso corresponde à matriz esparsa de precisão $C^{-1}$. Do ponto de vista computacional, é impraticável especificar uma priori hiper-inversa Wishart diretamente em $C^{-1}$. É realizada uma transformação em $C$ para fatorar a verossimilhança. A distribuição hiper inversa de Wishart i.e $C \sim \mathcal{HIW}_\mathcal G(\nu, \tau \mathbb I_s)$ transforma-se na variância escalar $\sigma^2_{qt}$ e no vetor de correlação associado $\boldsymbol \rho _{qt} = ( \rho _{1,qt},  \rho _{2,qt}, ...  ,\rho _{t-1,qt})^T$, com:

\begin{equation}
\sigma^2_{qt} \sim \mathcal{IG}amma \left(\frac{\nu - s + t + |S_q|}{2}, \frac \tau 2 \right), \  q = 1, ..., Q,\ t = 1,..., |R_q|, \boldsymbol \rho_{qt} | \sigma^2_{qt} \ 
\sim \mathcal N \left(\boldsymbol 0, \frac{\sigma^2_{qt}}{\tau} \mathbb I_{t-1}\right) \label{eq:5}
\end{equation}

onde $Q$ é o número de componentes primos no grafo decomposto $\mathcal G$, $S_q$ e $R_q$ são os separadores e os componentes residuais de $\mathcal G$, respectivamente.

Como *priori* para o grafo é utilizado uma Bernoulli com probabilidade $\eta$ em cada vértice $E_{kk'}$ de $\mathcal G$ como em:


\begin{equation}
\mathbb P (E_{kk'} \in \mathcal G) = \eta, \ \ \eta \sim \mathcal Beta (a_\eta, b_\eta). \label{eq:6}
\end{equation}

São admitidas três *prioris* em $\beta_\gamma$.

## Amostragem MCMC e inferência *a posteriori*


Para amostrar da distribuição a *posteriori*, os autores utilizam o algoritmo de busca estocástica evolucionária, que utiliza uma forma particular do Monte Carlo evolucionário (EMC).

Múltiplas cadeias de Markov temperadas são processadas paralelamente e movimentos de troca ou mudança são permitidos dentre as cadeias para melhorar a mistura entre modelos potencialmente diferentes da *posteriori*. A temperatura é adaptada durante a fase de burn-in.

A cadeia principal provém amostras da distribuição a *posteriori* não-temperada, que é utilizada para toda a inferência. Para cada variável resposta, os autores utilizam um amostrador de Gibbs para atualizar o vetor dos coeficientes de regressão $\beta_k(k = 1, ...,s)$, baseado na distribuição a *posteriori* condicional correspondente ao modelo específico, selecionado entre os modelos apresentados anteriormente.

Após $L$ iterações do MCMC, obtêm-se $\boldsymbol B^{(1)}, ..., \boldsymbol B^{(L)}$ e a estimativa da média a *posteriori* é:

$$ \hat{\boldsymbol B} = \frac{1}{L-b} \sum^L_{t = b+1} \boldsymbol B^{(t)}$$

onde $b$ é o número de iterações de *burn-in*. As distribuições condicionais completas a *posteriori* também estão disponíveis no modelo SSUR. Já nos modelos HRR, os coeficientes de regressão e as covariâncias residuais foram integrados para fora e ainda assim a saída do MCMC não pode ser utilizada diretamente para inferencia posterior desses parâmetros.

Contudo, para $\boldsymbol B$, a distribuição *posteriori* condicional em $\boldsymbol \Gamma$ pode ser obtida analiticamente nos modelos HRR, e é essa a saída oferecida.

Em cada iteração $t$ do MCMC também é atualizado cada vetor binário latente $\gamma_k(k=1,..., s)$ via Metropolis-Hastings, propondo conjuntamente uma atualização para o correspondente $\beta_k$. Após $L$ iterações, usando as matrizes binárias $\boldsymbol \Gamma^{(1)}, ..., \boldsymbol \Gamma^{(L)}$, as probabilidades de inclusão marginal a *posteriori* são estimadas por:

$$\hat{\boldsymbol \Gamma} = \frac{1}{L-b} \sum^L_{t = b+1} \boldsymbol \Gamma^{(t)}$$

Outro parâmetro importante dos modelos SSUR é $\mathcal G$ na *priori* Wisahrt hiper-inversa para a matriz de covariância $C$. Ela é atualizada via _junction tree sampler_ conjuntamente com a proposta correspondente para $\sigma^2_{qt}$ e $\boldsymbol \rho_{qt} | \sigma^2_{qt}$ em  \eqref{eq:5}.

A cada iteração do MCMC é extraída a matriz de adjacência $\mathcal G^{(t)} (t=1,...,L)$, do qual são derivadas as estimativas da média a *posteriori* das probabilidades de inclusão das areastas como:

$$\hat{\mathcal G} = \frac{1}{L-b} \sum^L_{t = b+1} \mathcal G^{(t)}$$

Mesmo que a *priori* o grafo $\mathcal G$ seja decomposto, a média estimada posteriormente $\hat{\mathcal G}$ pode estar pode do espaço de modelos decompostos.

O hiperparâmetro $\tau$ da Wishart hiper-inversa é atualizado através de um passeio aleatório do amostrador Metropolis-Hastings. Já $\eta$ e a variância $w$ na priori pico-e-tapa são amostrados das condicionais posteriores.

# Conjunto de Dados

Os autores simularam dados de polimorfismo de nucleotídeo único (SNP) dentro de um modelo verdadeiro conhecido para demonstrar a performance de recuperação dos modelos introduzidos anteriormente. O algoritmo completo pode ser encontrado em [@BayesSUR2021].

Para construir variáveis resposta múltiplas $\boldsymbol Y$ (com $s=10$) com uma relação estruturada, os autores fixam uma variável indicadora esparsa $\boldsymbol \Gamma$ e desenham um grafo decomposto para as respostas, para construir padrões de associação dentre os múltiplos regressores e variáveis resposta.

```{r data-load, echo=FALSE}
# pacote principal
library(BayesSUR)
data("exampleEQTL", package = "BayesSUR")
# attach nos dados para um código mais compacto
attach(exampleEQTL)
```

```{r prior-graphs,echo=FALSE, message = F, fig.cap="Parâmetros verdadeiros dos dados gerados no conjunto de dados de exemplo. Valores em branco representam 0 e, em azul, 1"}
# bibliotecas para gráficos
library(tidyverse)
library(gridExtra)
library(ggpubr)
library(tictoc)
# funcao que recebe uma matriz e plota um mapa de calor
 plot_heatmap<- function(df){
reshape2::melt(df) %>% ggplot(aes(x=Var1, y=Var2, fill=-value)) + 
    geom_raster() + guides(fill="none") +
    scale_fill_fermenter()
 }
# rotulos da escala
labs <- as_labeller(
          c("1" = "GEX1", "2" =  "GEX2",  "3" =  "GEX3", "4" = "GEX4","5" = "GEX5",
          "6" = "GEX6", "7" = "GEX7", "8" = "GEX8", "9" = "GEX9", "10" = "GEX10"))
# mapa de calor Y versus X
p <- plot_heatmap(gamma) + scale_y_continuous(breaks= 1:10,labels = labs)
# mapa de calor Y versus Y
q <- plot_heatmap(Gy)  + scale_x_continuous(breaks= 1:10,labels = labs)

grid.arrange(p,q, ncol = 2) 
```

Aqui, temos 10 colunas, que são as respostas em $\boldsymbol Y$, e 150 linhas, que são as regressores em $\boldsymbol X$. O segundo componente do exemplo é uma `blockList` que especifica os índices de $\boldsymbol Y$ e $\boldsymbol X$ em `data`.

O terceiro componente é a verdadeira matriz indicadora $\boldsymbol \Gamma$ dos coeficientes de regressão. O quarto componente é o grafo verdadeiro $\mathcal G$ entre as variáveis resposta. A Figura \ref(fig:prior-graphs) mostra os verdadeiros $\boldsymbol \Gamma$ e $\mathcal G$ utilizados para simular ao exemplo. As matrizes de associação são exibidas na forma de mapas de calor.


# Análise dos Dados

Os autores utilizam o exemplo para ajustar um modelo SSUR com uma *priori* *hotspot* para as variáveis indicadoras $\boldsymbol \Gamma$ e a *priori* indutora de esparsidão Wishart hiper-inversa utilizando o pacote `BayesSUR`. No exemplo são 200000 iterações do MCMC, com burn-in de 100000, em três cadeias paralelamente. Para manter a reprodutibilidade, o código foi executado em um único núcleo.

```{r model-fit, echo= FALSE}
set.seed(28173)
tic("Tempo de ajuste do modelo")
# ajuste do modelo
fit <- BayesSUR(data = data, Y = blockList[[1]], X = blockList[[2]],
                outFilePath = "results", nIter = 5000, nChains = 3,
                burnin = 1000, covariancePrior = "HIW",
                gammaPrior = "hotspot",
                output_CPO = TRUE,
                )
toc()
```

As Figuras \ref(fig:plt-beta), \ref(fig:plt-gamma) e \ref(fig:plt-G) resumem os resultados da inferência a posteriori dos estimadores $\hat{\boldsymbol B}, \hat{\boldsymbol \Gamma}, \text{e} \ \hat{\mathcal G}$.


```{r plt-beta, echo  = FALSE, fig.cap = "Coeficientes estimados da matriz $\\hat{\\boldsymbol B}$"}
# plota estimador beta
plotEstimator(fit, "beta")
```

```{r plt-gamma, echo  = FALSE, fig.cap = "Coeficientes estimados da matriz indicadora latente $\\hat{ \\boldsymbol \\Gamma}$"}
# plota estimador gamma
plotEstimator(fit, "gamma")
```

```{r plt-G, echo  = FALSE, fig.cap = "Esturutura aprendida de $\\hat{\\mathcal G}$ pelo modelo SSUR com priori hotspot e covariância esparsa"}
# plota o grafo das respostas
plotEstimator(fit,"Gy")
```

Vemos que o modelo SSUR possui uma boa recuperação do verdadeira matriz indicadora latente $\boldsymbol \Gamma$ e da estrutura das respostas representada por $\mathcal G$. Podemos comparar a verdadeira estrutura com a estimada quando limitamos a probabilidade de seleção a *posteriori* para $\boldsymbol \Gamma$ e $\mathcal G$ a 0.5, criando o grafo exibido à direita na \ref(fig:prior-graphs).


```{r plt-graphs, echo=FALSE, fig.height = 6, fig.width = 12, fig.cap="Estrutura estimada das dez variáveis resposta com $\\hat{\\mathcal G}$ limitado a 0.5 (esquerda). Estrutura verdadeira de $\\mathcal G$ representada pela matriz de adjacência `Gy` (direita)."}
# estrutura estimada vs estrutura verdadeira
layout(matrix(1:2, ncol = 2))
plot(fit, estimator = "Gy", type = "graph")
plotGraph(Gy)
```


Utilizando o mesmo limiar, de $\hat{\mathcal G}$ limitado a 0.5, e $\hat{\boldsymbol \Gamma}$ limitado a 0.5, podemos visualizar a rede completa com as dez expressões gênicas e os 150 SNPs na \ref{fig:plot-expr}.


```{r plot-expr, echo = FALSE, fig.width=10, fig.height=10, fig.cap="Representação da rede entre as expressões gênicas ($\\boldsymbol Y$), com $\\hat{\\mathcal G}$ limitado a 0.5, e os SNPs ($\\boldsymbol X$), com $\\hat{\\boldsymbol \\Gamma}$ limitado a 0.5."}
# estrutura entre X e Y
plot(fit, estimator = c("gamma", "Gy"), type = "network",
     name.predictors = "SNPs", name.responses = "Gene expression")
```

Além disso, os plots estilo Manhattan na Figura \ref{fig:plot-manh} mostram as probabilidades de inclusão marginal (mPIP) dos SNPs (painel superior) e o número de expressões gênicas da resposta associado com cada SNP (painel inferior). O número de respostas é baseado em $\hat{\boldsymbol \Gamma}$ limitado a 0.5.

```{r plot-manh, echo = FALSE, fig.width=8, fig.height=7, fig.cap="Plots estilo Manhattan, mostrando as probabilidades de inclusão marginal posterior (em cima) e o número de expressões gênicas da resposta associada a cada SNP (embaixo)."}
plot(fit, estimator = "gamma", type = "Manhattan")
```

Para investigarmos o comportamento do nosso amostrador MCMC, podemos utilizar os plots de diagnóstico apresentados na \ref{fig:plot-diag}. Observa-se que as cadeias de Markov utilizadas aparentemente começam a amostrar da distribuição correta após aproximadamente 50.000 iterações.

Os painéis inferiores da Figura \ref{fig:plot-diag} indicam que o logarítmo da distribuição a posteriori da variável indicadora latente $\boldsymbol \Gamma$ é estável para a última metade das cadeias, após subtraído o burn-in.

```{r plot-diag, echo = FALSE, fig.width=10, fig.height=8}
plot(fit, estimator = "logP", type = "diagnostics")
```

Embora não tenhamos acesso direto às cadeias simuladas, podemos também visualizar a verossimilhança das outras matrizes estimadas pelo modelo conforme mostra a Figura \ref{fig:lik-est}.

```{r lik-est, echo = FALSE, fig.cap="Verossimilhança dos estimadores pelo número de iterações do MCMC."}
plotMCMCdiag(fit, HIWg = "lik")
```


É também possível utilizar o CPO para encontrar observações destoantes, como uma forma de validação cruzada do modelo com a amostra obtida, conforme mostra a Figura \ref{fig:plot-cpo}. Temos algumas poucas observações abaixo do limiar.

```{r plot-cpo, echo = F, fig.cap="CPOs escalonados para o modelo de regressão ajustado"}
# CPO
plotCPO(fit)
```

Finalizamos nossa análise com um sumário do modelo obtido. Após isso, desatachamos o conjunto de dados utilizado.

```{r model-summary, echo = FALSE}
# sumario do modelo
summary(fit)
```

```{r detach, echo = FALSE}
detach(exampleEQTL)
```

# Conclusão

# Referências

<div id="refs"></div>

# Apêndice: códigos

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```