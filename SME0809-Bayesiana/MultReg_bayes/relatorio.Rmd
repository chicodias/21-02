---
title: "SME0809 - Inferência Bayesiana - Prova 2 - Grupo 13"
subtitle: "High-Dimensional Multivariate Bayesian Variable and Covariance Selection in Linear Regression [@BayesSUR2021]"
author:
  - "Francisco Miranda - 4402962"
  - "Heitor Carvalho - 11833351"
date: "Dezembro 2021"
institution: "ICMC - USP"
output: pdf_document
bibliography: packages.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introdução 

Com o desenvolvimento de técnicas de alto processamento na biologia molecular, a caracterização molecular em alta escala tornou-se um lugar comum, com o advento de técnicas como:

 - genome-wide measurement of gene expression
 - single nucleotide polymorphisms
 - CpG methylation status
 - pharmacological profiling for large-scale cancer drug screen.
 
A análise de associações conjuntas entre múltiplos fenótipos correlacionados e atributos moleculares de alta dimensionalidade é desafiadora.


Quando múltiplos fenótipos e informação genômica de alta dimensionalidade são analisados conjuntamente, a abordagem bayesiana permite especificar de maneira flexível as relações complexas entre os conjunto de dados altamente estruturados.

O pacote `BayesSUR` combina diversos modelos que foram propostos para a regressão multidimentional com resposta múltipla e introduz um novo modelo, que permite diferentes *prioris* na seleção de variáveis dos modelos de regressão e para diferentes pressupostos a respeito da estrutura de dependência entre as respostas.


# Metodologia

 - múltiplas opções de seleção de variáveis
 - a matriz de covariância pode ser diagonal, densa ou esparsa.
 - engloba três classes de modelos de regressão linear de múltipla resposta:
  - HRR
  - dSUR e SSUR
  - MRF
 
 
 O modelo de regressão é escrito como:
 
\begin{equation}
\boldsymbol{Y} = \boldsymbol{XB} + \boldsymbol{U} \label{eq:1}
\end{equation}

$$\text{vec}{(\boldsymbol{U})} \sim \mathcal{N}(\boldsymbol{0}, C \otimes \mathbb{I}_n) $$

onde:

 - $\boldsymbol{Y}$ é uma matriz $s \times s$ das variáveis resposta com matriz de covariância C;
 - $\boldsymbol{X}$ é uma matriz $n \times p$ de preditores para todas as respostas;
 - $\boldsymbol{U}$ é a matriz dos resíduos;
 - $\text{vec}(\cdot)$ denota a vetorização da matriz;
 - $\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$ denota uma distribuição normal multivariada com vetor de médias $\boldsymbol{\mu}$ e matriz de covariâncias $\boldsymbol{\Sigma}$;
 - $\boldsymbol{0}$ denota um vetor coluna com todos os elementos nulos,
 - $\otimes$ é o produto de Kronecker e $\mathbb{I}_n$ a matriz identidade de ordem $n$.


A seleção de variáveis é realizada através de uma matriz indicadora binária latente $\boldsymbol{\Gamma} = \{\gamma_{jk}\}$.

Uma *priori* "spike-and-slab" é utilizada para encontrar um subconjunto esparso relevante de preditores que expliquem a variabilidade de $\boldsymbol{Y}$: condicional em $\gamma_{jk} = 0\ \ (j = 1, ..., p, \text{e}\ k=1, ..., s)$

Definem-se $\beta_{jk} = 0$ condicionado em $\gamma_{jk} = 1$ seguem uma distribuição normal difusa:

\begin{equation}
\beta_\gamma \sim \mathcal{N}(\textbf{0}, W^{-1}_\gamma) \label{eq:2}
\end{equation}

Onde $\beta = \text{vec}(\textbf{B})$, $\gamma = \text{vec}( \boldsymbol{\Gamma})$, $\beta_\gamma$ consiste somente nos coeficientes selecionados (i.e. $\gamma_{jk} = 1$), assim $W_\gamma$ é a sub matriz de W formada pelos coeficientes selecionados correspondentes.

A matriz de precisão, W, é geralmente decomposta em coeficientes de encolhimento e uma matriz que governa a estrutura de covariância dos coeficientes de regressão. É utilizado aqui $W = w^{-1} \mathbb{I}_{sp}$, o que significa que todos os coeficientes de regressão são independentes a priori, com uma *hiperpriori* no coeficiente de encolhimento $w$, i.e. $w \sim \mathcal{IG}\text{amma}(a_w, b_w)$.

A matriz indicadora binária latente $\boldsymbol \Gamma$ tem três opções de *priori*:

  - Bernoulli independente hierárquica
  - hotspot prior
  - MRF prior
  
A matriz de covariância $C$ também possui três *prioris*:

  - Gama inversa independente
  - Wishart inversa
  - hiper-inversa Wishart
  
São considerados no total nove possíveis modelos dentre as combinações de $C$ e $\boldsymbol \Gamma$

|  | ${\gamma_{jk}} \sim Bernoulli$ | $\gamma_{jk} \sim$ hotspot | $\gamma_{jk} \sim$ MRF |
|:---:|:---:|:---:|:---:|
| $C\sim indep$ | HRR-B | HRR-H | HRR-M |
| $C\sim IW$ | dSUR-B | dSUR-H | dSUR-M |
| $C\sim HIW$ | SSUR-B | SSUR-H | SSUR-M |

##  Regressão Hierárquica Relacionada (HRR)

A Regressão Hierárquica Relacionada assume que $C$ é uma matriz diagonal, o que se traduz em independência condicional entre múltiplas variáveis resposta.

Uma *priori* gama inversa é especificada para a covariância dos resíduos, i.e

$$\sigma^2_k \sim \mathcal{IG}\text{amma}(a_\sigma, b_\sigma)$$

Quando combinada com as *prioris* em \eqref{eq:2}, é conjulgado com a verossimilhança do modelo \eqref{eq:1}. Podemos então amostrar a estrutura de seleção de variáveis $\boldsymbol{\Gamma}$ marginalmente com respeito a $C$ e $\boldsymbol B$.

### HRR com uma *priori* Bernouli independente

Para uma *priori* simples de seleção do modelo de regressão, os indicadores binários latentes seguem uma *priori* de Bernoulli:

\begin{equation}
\gamma_{jk}|\omega_{jk} \sim \mathcal Ber(\omega_{jk})\ \ (j = 1, ..., p, \text{e}\ k=1, ..., s)  \label{eq:3} 
\end{equation}


Com uma priori hierárquica Beta em $\omega_j$, i.e. $\omega_j \sim \mathcal Beta(a_\omega, b_\omega)$, que quantifica a probabilidade de cada preditor ser associado com qualquer uma das variáveis resposta.

### HRR com uma *priori* hotspot

É proposta a decomposição da probabilidade do parâmetro de associação $\omega_{jk}$ em \eqref{eq:3}, onde $o_k$ é responsável pela esparsividade de cada modelo de resposta e $\pi_j$ controla a propensão de cada preditor a ser associado a múltiplas respostas simuntaneamente:

\begin{equation}
\gamma_{jk}|\omega_{jk} \sim \mathcal Ber(\omega_{jk})\ \ (j = 1, ..., p, \text{e}\ k=1, ..., s)  \label{eq:4} 
\end{equation}
$$\begin{gathered}
\omega_{jk} = o_k \times \pi_j \\
o_k \sim \mathcal Beta (a_0, b_0) \\
\pi_j \sim \mathcal Gamma(a_\pi, b_\pi)
\end{gathered}$$

## Regressão não relacionada aparentemente esparsa (SSUR)

Para modelar a matriz de covariância $C$ é especificado uma *priori* hiper-Inversa Wishart, o que significa que as variáveis resposta têm por trás um grafo $\mathcal G$ que codifica a dependência condicional entre as respostas.

Um grafo esparso corresponde à matriz esparsa de precisão $C^{-1}$. Do ponto de vista computacional, é impraticável especificar uma priori hiper-inversa Wishart diretamente em $C^{-1}$. É realizada uma transformação em $C$ para fatorar a verossimilhança. A distribuição hiper inversa de Wishart i.e $C \sim \mathcal{HIW}_\mathcal G(\nu, \tau \mathbb I_s)$ transforma-se na variância escalar $\sigma^2_{qt}$ e no vetor de correlação associado $\boldsymbol \rho _{qt} = ( \rho _{1,qt},  \rho _{2,qt}, ...  ,\rho _{t-1,qt})^T$, com:

\begin{equation}
\sigma^2_{qt} \sim \mathcal{IG}amma \left(\frac{\nu - s + t + |S_q|}{2}, \frac \tau 2 \right), \  q = 1, ..., Q,\ t = 1,..., |R_q|, \boldsymbol \rho_{qt} | \sigma^2_{qt} \ 
\sim \mathcal N \left(\boldsymbol 0, \frac{\sigma^2_{qt}}{\tau} \mathbb I_{t-1}\right) \label{eq:5}
\end{equation}

onde $Q$ é o número de componentes primos no grafo decomposto $\mathcal G$, $S_q$ e $R_q$ são os separadores e os componentes residuais de $\mathcal G$, respectivamente.

Como *priori* para o grafo é utilizado uma Bernoulli com probabilidade $\eta$ em cada vértice $E_{kk'}$ de $\mathcal G$ como em:


\begin{equation}
\mathbb P (E_{kk'} \in \mathcal G) = \eta, \ \ \eta \sim \mathcal Beta (a_\eta, b_\eta). \label{eq:6}
\end{equation}

São admitidas três *prioris* em $\beta_\gamma$.

## Amostragem MCMC e inferência *a posteriori*


Para amostrar da distribuição a *posteriori*, os autores utilizam o algoritmo de busca estocástica evolucionária, que utiliza uma forma particular do Monte Carlo evolucionário (EMC).

Múltiplas cadeias de Markov temperadas são processadas paralelamente e movimentos de troca ou mudança são permitidos dentre as cadeias para melhorar a mistura entre modelos potencialmente diferentes da *posteriori*. A temperatura é adaptada durante a fase de burn-in.

A cadeia principal provém amostras da distribuição a *posteriori* não-temperada, que é utilizada para toda a inferência. Para cada variável resposta, os autores utilizam um amostrador de Gibbs para atualizar o vetor dos coeficientes de regressão $\beta_k(k = 1, ...,s)$, baseado na distribuição a *posteriori* condicional correspondente ao modelo específico, selecionado entre os modelos apresentados anteriormente.

Após $L$ iterações do MCMC, obtêm-se $\boldsymbol B^{(1)}, ..., \boldsymbol B^{(L)}$ e a estimativa da média a *posteriori* é:

$$ \hat{\boldsymbol B} = \frac{1}{L-b} \sum^L_{t = b+1} \boldsymbol B^{(t)}$$

onde $b$ é o número de iterações de *burn-in*. As distribuições condicionais completas a *posteriori* também estão disponíveis no modelo SSUR. Já nos modelos HRR, os coeficientes de regressão e as covariâncias residuais foram integrados para fora e ainda assim a saída do MCMC não pode ser utilizada diretamente para inferencia posterior desses parâmetros.

Contudo, para $\boldsymbol B$, a distribuição *posteriori* condicional em $\boldsymbol \Gamma$ pode ser obtida analiticamente nos modelos HRR, e é essa a saída oferecida.

Em cada iteração $t$ do MCMC também é atualizado cada vetor binário latente $\gamma_k(k=1,..., s)$ via Metropolis-Hastings, propondo conjuntamente uma atualização para o correspondente $\beta_k$. Após $L$ iterações, usando as matrizes binárias $\boldsymbol \Gamma^{(1)}, ..., \boldsymbol \Gamma^{(L)}$, as probabilidades de inclusão marginal a *posteriori* são estimadas por:

$$\hat{\boldsymbol \Gamma} = \frac{1}{L-b} \sum^L_{t = b+1} \boldsymbol \Gamma^{(t)}$$

Outro parâmetro importante dos modelos SSUR é $\mathcal G$ na *priori* Wisahrt hiper-inversa para a matriz de covariância $C$. Ela é atualizada via _junction tree sampler_ conjuntamente com a proposta correspondente para $\sigma^2_{qt}$ e $\boldsymbol \rho_{qt} | \sigma^2_{qt}$ em  \eqref{eq:5}.

A cada iteração do MCMC é extraída a matriz de adjacência $\mathcal G^{(t)} (t=1,...,L)$, do qual são derivadas as estimativas da média a *posteriori* das probabilidades de inclusão das areastas como:

$$\hat{\mathcal G} = \frac{1}{L-b} \sum^L_{t = b+1} \mathcal G^{(t)}$$

Mesmo que a *priori* o grafo $\mathcal G$ seja decomposto, a média estimada posteriormente $\hat{\mathcal G}$ pode estar pode do espaço de modelos decompostos.

O hiperparâmetro $\tau$ da Wishart hiper-inversa é atualizado através de um passeio aleatório do amostrador Metropolis-Hastings. Já $\eta$ e a variância $w$ na priori pico-e-tapa são amostrados das condicionais posteriores.

# Conjunto de Dados

Os autores simularam dados de polimorfismo de nucleotídeo único (SNP) dentro de um modelo verdadeiro conhecido para demonstrar a performance de recuperação dos modelos introduzidos anteriormente. O algoritmo completo pode ser encontrado em [@BayesSUR2021].

Para construir variáveis resposta múltiplas $\boldsymbol Y$ (com $s=10$) com uma relação estruturada, os autores fixam uma variável indicadora esparsa $\boldsymbol \Gamma$ e desenham um grafo decomposto para as respostas, para construir padrões de associação dentre os múltiplos regressores e variáveis resposta.

```{r data-load, echo=FALSE}
library(BayesSUR)
data("exampleEQTL", package = "BayesSUR")
attach(exampleEQTL)
```

```{r prior-graphs,echo=FALSE, message = F, fig.cap="Parâmetros verdadeiros dos dados gerados no conjunto de dados de exemplo"}
library(tidyverse)
library(gridExtra)

 plot_heatmap<- function(df){
reshape2::melt(df) %>% ggplot(aes(x=Var1, y=Var2, fill=value)) + 
    geom_raster() + guides(fill="none")
 }
  
 
p <- plot_heatmap(gamma)
q <- plot_heatmap(Gy)

grid.arrange(p,q, ncol = 2) 
```




# Análise dos Dados

```{r model-fit, echo= FALSE, message = FALSE, }
fit <- BayesSUR(data = data, Y = blockList[[1]], X = blockList[[2]],
                outFilePath = "results", nIter = 5000, nChains = 3,
                burnin = 1000, covariancePrior = "HIW",
                gammaPrior = "hotspot",
                output_CPO = TRUE,
                )
```

```{r responses-est-graph, echo = FALSE, out.width="100%", out.height="100%"}
plot(fit, estimator = c("beta", "gamma", "Gy"), type = "heatmap",
     fig.tex = FALSE)
```

```{r echo=FALSE, fig.height = 6, fig.width = 12}
layout(matrix(1:2, ncol = 2))
plot(fit, estimator = "Gy", type = "graph")
plotGraph(Gy)
```

```{r plot-expr, echo = FALSE, fig.width=10, fig.height=10}
plot(fit, estimator = c("gamma", "Gy"), type = "network",
     name.predictors = "SNPs", name.responses = "Gene expression")
```

```{r plot-manh, echo = FALSE, fig.width=8, fig.height=7}
plot(fit, estimator = "gamma", type = "Manhattan")
```


```{r plot-diag, echo = FALSE, fig.width=10, fig.height=8}
plot(fit, estimator = "logP", type = "diagnostics")
```
```{r}
plotCPO(fit)
```

```{r}
plotEstimator(fit, "gamma")
```

```{r}
BayesSUR::plotEstimator(fit, "beta")
```

```{r}
plotEstimator(fit,"Gy")
```

```{r}
plotMCMCdiag(fit, HIWg = "lik")
```

```{r}
summary(fit)
```


# Conclusão

# Referências

<div id="refs"></div>

# Apêndice: códigos

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```