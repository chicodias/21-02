---
title: "SME0820 - Modelos de Regressão e Aprendizado Supervisionado I - Exercício I"
author:
  - Brenda da Silva Muniz 11811603
  - Francisco Rosa Dias de Miranda 4402962
  - Heitor Carvalho Pinheiro 11833351
  - Mônica Amaral Novelli 11810453
date: "Setembro 2021"
output: pdf_document
---

# Exercício 2

Queremos mostrar que os estimadores $\hat{\beta_0}$ e $\hat{\beta_1}$ são não enviesados.


$$E(\hat{\beta_1}) = E \left(\frac{S_{XY}}{S_{XX}}\right) =
E \left(\frac{\sum_{i=1}^n(x_i - \bar{x})y_i}{\sum_{i=1}^n(x_i - \bar{x})x_i} \right) =
\frac{\sum_{i=1}^n(x_i - \bar{x})E(y_i)}{\sum_{i=1}^n(x_i - \bar{x})x_i} $$

Temos que $y_i = \beta_0 + \beta_1x_i + \epsilon_i, \ i = 1,...,n$. Assim:

$$E(y_i) = E(\beta_0 + \beta_1x_i + \epsilon_i) =
E(\beta_0) + E(\beta_1x_i) + E(\epsilon_i) = \beta_0 + \beta_1x_i$$

Voltando a expressão original, temos que:

$$E(\hat{\beta_1}) = \frac{\sum_{i=1}^n(x_i - \bar{x}) (\beta_0 + \beta_1x_i)}{\sum_{i=1}^n(x_i - \bar{x})x_i} =
\frac{\beta_0 \sum_{i=1}^n(x_i - \bar{x})}{\sum_{i=1}^n(x_i - \bar{x})x_i} + \frac{\beta_1\sum_{i=1}^n(x_i - \bar{x}) x_i}{\sum_{i=1}^n(x_i - \bar{x})x_i} = \beta_1.$$

Procedendo de forma análoga para $\beta_0$, temos que:

$$E(\hat{\beta_0}) = E(\bar{y} - \hat{\beta_1}\bar{x}) =
E(\bar{y}) - \bar{x}E(\hat{\beta_1})$$

Resta-nos obter

$$E(\bar{y}) = E \left(\frac1n \sum_{i=1}^n y_i \right) =
\frac1n \sum_{i=1}^n E(y_i)  =
\frac1n \sum_{i=1}^n (\beta_0 + \beta_1 x_i)  =$$

$$\frac{n\beta_0}n + \beta_1 \frac{\sum_{i=1}^n x_i}n =
\beta_0 + \beta_1\bar{x}$$

Segue que:

$$E(\hat{\beta_0}) = E(\bar{y}) - \bar{x}E(\hat{\beta_1}) =
(\beta_0 + \beta_1\bar{x}) - \beta_1\bar{x}= \beta_0$$

Portanto, $\hat{\beta_1}$ e $\hat{\beta_0}$ são não enviesados.

# Exercício 3

Queremos obter o $EEMQ$ para o modelo linear simples sem intercepto $\beta_0$.

Nesse caso, temos a equação:

$$ y_i = \beta_1x_i + \epsilon $$
Definindo um estimador $\hat{\beta_1}$ para $\beta_1$:


Queremos que $\sum_{i=1}^n\epsilon_i^2 = 0$ Dado que $\epsilon_i = y_i - \beta_1x_i$

Teremos que:

$$ S(\hat\beta_1) = \sum_{i=1}^n(y_i - \hat\beta_1x_i)^2 = \sum_{i=1}^ny_i^2 - 2\hat\beta_1\sum_{i=1}^ny_ix_i + \hat\beta_1^2\sum_{i=1}^nx_i^2$$

Minimizando o valor de $S(\beta_1)$ e igualando a zero:

$$\frac{\partial S(\hat\beta_1)}{\partial \hat\beta_1} = 0 \Rightarrow\frac{\partial S(\hat\beta_1)}{\partial \hat\beta_1} = -2\sum_{i=1}^ny_ix_i + 2\hat\beta_1\sum_{i=1}^nx_i^2 = 0$$
Logo:

$$\hat\beta_1 = \sum_{i=1}^n\left(\frac{y_ix_i}{x_i^2} \right)$$
Para mostrar que $\hat\beta_1$ é não-viesado, precisamos mostrar que $E(\hat\beta_1) = \beta_1$.

Segue que:

$$E(\hat\beta_1) = \frac{1}{\sum_{i=1}^nx_i^2} \sum_{i=1}^nx_i E(y_i) = \frac{1}{\sum_{i=1}^nx_i^2} \sum_{i=1}^nx_i (\beta_1x_i) = \frac{\sum_{i=1}^n x_i^2}{\sum_{i=1}^n x_i^2} \beta_1 = \beta_1 $$

$$ \therefore~ E(\hat\beta_1) = \beta_1$$
Por fim, podemos confirmar que este ponto é, de fato, o ponto de mínimo global, se conseguirmos mostrar que:

$$
\frac{\partial^2 S(\hat\beta_1)}{\partial \hat\beta_1^2} >0
$$
Para isso, segue que:
$$
\frac{\partial~ (-2\sum_{i=1}^ny_ix_i + 2\hat\beta_1\sum_{i=1}^nx_i^2)}{\partial(\hat\beta_1)} = 2\sum_{i=1}^nx_i^2>0
$$
Desse modo, o $EMMQ$ $\hat\beta_1$ para o modelo linear simples sem o intercepto é não-viesado.

# Exercício 4

Queremos mostrar que $SQ_{total} = \sum_{i=1}^n Y_i^2$ no modelo linear simples sem o intercepto, pode ser escrito como $SQ_{total} = \beta_1 \sum_{i=1}^n X_i Y_i + SQ_{res}$.

Como o modelo não tem intercepto, temos que $\hat Y_i = \hat\beta_1 X_i$

<<<<<<< HEAD
$$SQ_{res} = \sum_{i=1}^n (Y_i - \hat Y_i)^2 = \sum_{i=1}^n (Y_i^2 - 2 Y_i \hat Y_i + \hat Y_i^2) = \sum_{i=1}^n Y_i^2 - 2 \sum_{i=1}^n Y_i(\hat\beta_1 X_i) + \sum_{i=1}^n (\hat\beta_1 X_i)^2 $$
=======
$$SQ_{res} = \sum_{i=1}^n (Y_i - \hat Y_i)^2 = \sum_{i=1}^n (Y_i^2 - 2 Y_i \hat Y_i + \hat Y_i^2) = \sum_{i=1}^n Y_i^2 - 2 \sum_{i=1}^n Y_i(\hat\beta_1 X_i) + \sum_{i=1}^n (\hat\beta_1 X_i)^2$$
>>>>>>> 5c406cdc8c939c92740cecebe4953c48bc7fe2ad

Temos que $SQ_{total} = \sum_{i=1}^n Y_i^2$

Logo, até agora, nossa equação é:

<<<<<<< HEAD
$$SQ_{res} = SQ_{total} - 2 \sum_{i=1}^n Y_i(\hat\beta_1 X_i) + \sum_{i=1}^n (\hat\beta_1 X_i)^2= SQ_{total} - 2 \sum_{i=1}^n \hat\beta_1 X_iY_i + \sum_{i=1}^n \hat\beta_1^2 X_i^2= SQ_{total} - 2 \hat\beta_1 \sum_{i=1}^n  X_iY_i + \hat\beta_1^2 \sum_{i=1}^n X_i^2 $$

Isolando $SQ_{total}$ temos que:

$$SQ_{total} = SQ_{res} + 2 \hat\beta_1 \sum_{i=1}^n  X_iY_i - \hat\beta_1^2 \sum_{i=1}^n X_i^2 = SQ_{res} +\hat\beta_1 (2\sum_{i=1}^n  X_iY_i - \hat\beta_1^2 \sum_{i=1}^n X_i^2) = SQ_{res} +\hat\beta_1 (2\hat\beta_1\sum_{i=1}^n  X_i^2 - \hat\beta_1 \sum_{i=1}^n X_i^2) = SQ_{res} +\hat\beta_1 (\hat\beta_1\sum_{i=1}^n  X_i^2) = SQ_{res} +\hat\beta_1\sum_{i=1}^n  X_iY_i $$

Assim sendo, está provado.
=======
$$SQ_{res} = SQ_{total} - 2 \sum_{i=1}^n Y_i(\hat\beta_1 X_i) + \sum_{i=1}^n (\hat\beta_1 X_i)^2$$
>>>>>>> 5c406cdc8c939c92740cecebe4953c48bc7fe2ad
