---
title: "SME0820 - Modelos de Regressão e Aprendizado Supervisionado I - Exercício 2"
author:
  - Brenda da Silva Muniz 11811603
  - Francisco Rosa Dias de Miranda 4402962
  - Heitor Carvalho Pinheiro 11833351
  - Mônica Amaral Novelli 11810453
date: "Novembro de 2021"
output: pdf_document
---


Este trabalho tem como objetivo ajustar um modelo de regressão linear múltipla a um conjunto de dados.

## Conjunto de dados

O dataset contém dados de um experimento para determinar **pressão**, **temperatura**, **fluxo de CO2**, **umidade** e **tamanho da partícula de amendoim** sob o **rendimento total de aceite por lote de amendoim**. [rendimento (y)].

Significância: 97%


```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
library(ggpubr)
library(gridExtra)
library(GGally)
library(pander)
```

```{r, echo=TRUE, warning=FALSE}
dados <- read_csv("dados/data-table-B7.csv", locale = locale(decimal_mark = ","))
dim(dados)

#Renomeando as colunas
names(dados) <- c('Pressao', 'Temp', 'FluxoCO2', 'Umidade', 'Tamanho', 'y')

head(dados)
```
Temos cinco covariáveis e a coluna $y$ corresponde a nossa variável preditora que determina **o rendimento total de azeite por lote de amendoim**


## 1. Análise Descritiva dos dados

Para facilitar nosso trabalho em termos computacionais, estaremos nomeando nossas variáveis e criando um data frame com as mesmas; sendo:

- $Y:$ Rendimento total de azeite por lote de amendoim*

- $X_1:$ Tamanho

- $X_2:$ Temperatura

- $X_3:$ Pressão

- $X_4:$ Fluxo de CO2

- $X_5:$ Umidade

```{r}
x1 <- dados$Pressao
x2 <- dados$Temp
x3 <- dados$FluxoCO2
x4 <- dados$Umidade
x5 <- dados$Tamanho
y <- dados$y

tabela01 <- data.frame(cbind(x1, x2, x3, x4, x5, y))
```


Devido à complexidade das fórmulas envolvidas para realizarmos uma regressão linear múltima, utilizaremos uma abordagem matricial. Desse modo, poderemos escrever

$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_k x_{ki} + \varepsilon_i, i = 1, ..., 5. $

como:

$y1 = \beta_0 + \beta_1 x_{11} + \beta_2 x_{21} + ... + \beta_k x_{k1} + \varepsilon_1$
$y2 = \beta_0 + \beta_1 x_{12} + \beta_2 x_{22} + ... + \beta_k x_{k2} + \varepsilon_2$
$y3 = \beta_0 + \beta_1 x_{13} + \beta_2 x_{23} + ... + \beta_k x_{k3} + \varepsilon_3$
$y4 = \beta_0 + \beta_1 x_{14} + \beta_2 x_{24} + ... + \beta_k x_{k4} + \varepsilon_4$
$y5 = \beta_0 + \beta_1 x_{15} + \beta_2 x_{25} + ... + \beta_k x_{k5} + \varepsilon_5$

Assim, alocamos essas equações em dois vetores colunas (5x1), fazendo:

```{r}

n <- length(dados$y)

X <- matrix(c(rep(1,n), x1, x2, x3, x4, x5), ncol = 6, nrow = n, byrow = FALSE)

Y <- matrix(y, ncol = 1, nrow = n)

k <- ncol(X) -1

p <- k + 1
```


Uma vez definidos tais pontos, podemos iniciar nossa análise descritiva dos dados, fazendo a montagem dos gráficos visando o estudo de como tais variáveis se comportam. 


*Teste de Correlação de Pearson entre as covariáveis e $y$*

  Na tabela abaixo, podemos observar as correlações de todas as covariáveis entre si e com a variável preditora. Analisando esses resultados, vemos que nenhuma das covariáveis se correlacionam entre si, e apresentam uma correlação muito baixa com a variável preditora - com exceção de x5 (Tamanho) com y.

```{r}

tabela_correlacao <- round(stats::cor(tabela01), 5)

tabela_correlacao
```

Fazendo uso das correlações, podemos dispor graficamente uma matriz de gráficos para expor as relações entre as variáveis, de modo que teremos histogramas de frequência nas diagonais, gráficos de dispersão no painel triangular inferior e  coeficientes das correlações no superior, de modo que o tamanho dos números é condicionado ao valor da correlação.

```{r}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y))
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste0(prefix, txt)
  if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex = cex.cor * r)
}

panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}


pairs(dados, diag.panel = panel.hist, upper.panel = panel.cor,
      lower.panel = panel.smooth)

```
### Histogramas

```{r}
h1 <- ggplot(tabela01, aes(x1)) + geom_histogram() +
  xlab("Pressao") + ylab("Frequencia")

h2 <- ggplot(tabela01, aes(x1)) + geom_histogram() +
  xlab("Temperatura") + ylab("Frequencia")

h3 <- ggplot(tabela01, aes(x1)) + geom_histogram() +
  xlab("Fluxo de CO^2") + ylab("Frequencia")

h4 <- ggplot(tabela01, aes(x1)) + geom_histogram() +
  xlab("Umidade") + ylab("Frequencia")

h5 <- ggplot(tabela01, aes(x1)) + geom_histogram() +
  xlab("Tamanho") + ylab("Frequencia")

```


```{r}
ggplot(tabela01, aes(x = "", y = x1)) +
    geom_boxplot(color = "blue")
```


```{r message =F}
<<<<<<< HEAD
dados %>% pivot_longer(!y) %>%
  ggplot(aes(x = value, y = y)) + geom_point() +
  geom_smooth(formula = y ~ x, method = "lm") +
  facet_wrap(~name, scales = "free_x") +
  theme_pubclean()
=======
dados %>% pivot_longer(!y) %>% 
  ggplot(aes(y = y)) + geom_point(aes(x = value)) + 
  facet_wrap(~name, scales = "free_x") 
```

```{r}
dados %>% pivot_longer(!c(Tamanho, y)) %>% 
  ggplot(aes(y = y, color = as_factor(Tamanho))) + geom_point(aes(x = value)) + 
  facet_wrap(~name, scales = "free_x") 

dados %>% pivot_longer(!y) %>%
  ggplot(aes(y = y)) + geom_point(aes(x = value)) +
  facet_wrap(~name, scales = "free_x")
>>>>>>> 383271d1c1467286d90fea2b1b7d9449315b7777
```

```{r}
dados %>% pivot_longer(!c(Tamanho, y)) %>%
  ggplot(aes(y = y, color = as_factor(Tamanho))) + geom_point(aes(x = value)) +
  facet_wrap(~name, scales = "free_x")
```


```{r}
dados %>% pivot_longer(cols = everything()) %>%
  ggplot() + geom_bar(aes(x = as_factor(value)),stat="count") +
  facet_wrap(~name, scales = "free_x")
```

```{r}
GGally::ggparcoord(dados[,c(2, 4, 6, 1, 3)])
```

```{r}
GGally::ggcorr(dados)
```

```{r message = F}
GGally::ggpairs(dados, columns = c(1:4,6), aes(colour = as_factor(Tamanho)))
```

## 2. ajuste do modelo de Regressão Linear Múltipla


Ajustaremos um modelo de regressão utilizando todas as covariáveis presentes em nosso conjunto de dados.

Definimos um modelo de regressão linear múltipla com $k$ covariáveis através da fórmula $Y=\beta_0+\beta_1X_1+\ldots+\beta_kX_k+\xi$, onde $\beta_j, j=1,\ldots,k$ são os coeficientes de regressão e representam a mudança esperada na variável resposta $Y$ por unidade de mudança em $X_i$ quando todas as outras covariáveis $X_i (i\neq j)$ são mantidas constantes.

Podemos também representar o modelo da seguinte forma:

$$\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\xi}$$

Onde:

$\boldsymbol{Y}=$ Vetor de $(n\times 1)$ observações;

$\boldsymbol{X}=$ Matriz $(n\times p)$ de covariáveis;

$\boldsymbol{\beta=}$ Vetor $P\times 1$ de coeficientes de regressão;

$\xi=$ Vetor n\time 1 de erros aleatórios;

$p=k+1$.


```{r}
#Definindo os betas do modelo de regressão múltipla
betas <- solve(t(X)%*%X)%*%t(X)%*%Y
betas

#Definindo a matrix C_jj
C_jj = solve(t(X)%*%X)

#Definindo uma matrix para os betas
betas <- matrix(data = betas, nrow = length(betas), ncol = 1, byrow = FALSE)
rownames(betas) <- c("beta0", "beta1","beta2","beta3","beta4","beta5")
betas

#Modelo do R
lm(formula = y ~., data = dados)
```


## 3. Estimação de $\sigma^2$

$SQ_{res}=\boldsymbol{Y}^{T}\boldsymbol{Y}-\widehat{\boldsymbol{\beta}}^{T}\boldsymbol{X}^{T}\boldsymbol{Y}$

```{r}
SQRes <- (t(Y)%*%Y)-(t(betas)%*%t(X)%*%Y)
SQRes
```
Logo, $\widehat{\sigma}^2=\dfrac{SQres}{n-p}$

```{r}
p <- ncol(X)

#estimando o sigma^2
sigma2 <- SQRes/(n-p)
sigma2
```


## 4. ANOVA

Agora que ajustamos um modelo inicial, é necessário que verifiquemos se ele é adequado em explicar a variabilidade de nossa amostra. Vamos assumir que $\xi\sim N_n(0,\sigma^2I)$. Precisamos agora verificar nossa suposição graficamente:

```{r}

# Obtendo uma estimativa para Y a partir do modelo ajustado
Y_est <- X%*%betas

# Cálculo dos resíduos
res <- Y - Y_est
```

```{r}
p <- ggplot(tibble(res), aes(sample = res)) + stat_qq() + stat_qq_line() +
  labs(x = "Amostra",
       y = "Quantis Teóricos",
       title = "Normal Q-Q Plot") +
  theme_pubclean()

q <- ggplot(tibble(res), aes(res)) +
  geom_histogram(aes(y=..density..), binwidth = 4, stat = "bin") +
  labs(title = "Histograma dos resíduos",
       y = "Densidade",
       x = "Valor") +
  theme_pubclean()

grid.arrange(p,q, ncol = 2)

```

O Q-Q Plot nos mostra o quanto os resíduos estão distantes do esperado dado que os dados têm distribuição normal, enquanto que o histograma dos resíduos nos fornece aproximações a respeito da distribuição de $\xi$.

A partir dos gráficos acima, podemos notar que os resíduos não estão muito afastados dos quantis teóricos, embora sua distribuição seja ligeiramente assimétrica, conforme constatado no histograma. Podemos também utilizar o teste de Shapiro-Wilk para verificar a normalidade dos dados.

```{r}
pander(shapiro.test(res), style='rmarkdown',
       caption = "Teste de normalidade Shapiro-Wilk para os resíduos")
```


O teste acima confirma nossa suposição de que os resíduos têm distribuição Normal, pois, para um nível de significância de 97%, o valor-p obtido, 0,2754, não rejeita a hipótese nula, de normalidade dos dados.

Além disso, para determinar matematicamente se existe uma relação linear entre a variável resposta $\boldsymbol{Y}$ e qualquer as outras covariáveis $\boldsymbol{X}_1,\ldots,\boldsymbol{X}_k$, é possível utilizar o teste **ANOVA**. Nele, queremos testar:

**$H_0$**: Nenhuma das variáveis contribui significativamente ao modelo, versus:

**$H_a$**: Pelo menos uma das covariáveis contribui significativamente ao modelo.


Tabela *ANOVA*:

| **F.V.** | **G.L** | **S.Q.** | **Q.M.** | **F** |
|:---:|:---:|:---:|:---:|:---:|
| **Regressão** | $k$ | $SQ_{reg}$ | $QMreg=\dfrac{SQreg}{k}$ | $F=\dfrac{QMreg}{QMres}$ |
| **Resíduo** | $n-p$ | $SQ_{res}$ | $QMres=\dfrac{SQres}{n-p}$ |  |
| **Total** | $n-1$ | $SQ_{total}$ |  $QM_{Total}$  |  |
Table: Tabela ANOVA


```{r}
# Soma dos quadrados dos residuos  
(SQRes <- t(Y-Y_est)%*%(Y-Y_est))
```


$$SQ_{Reg} = SQ_{Total} - SQ_{Res} = \frac 1n \sum_{i=1}^n y_i^2 - (Y-\hat{Y})^T\cdot(Y-\hat{Y}) =$$
$$\beta^T \cdot X^T \cdot Y - \frac1n (u^T \cdot Y)^2$$

```{r}
# Soma dos quadrados totais
u <- c(rep(1,n))
(SQTotal <- t(Y)%*%Y - ((t(u)%*%Y)^2)/n)
```

```{r}
#Soma dos quadrados da regressao  
(SQReg <- SQTotal - SQRes)
```


```{r}
#Calculando a anova
k <- 2 # covariaveis utilizadas
p <- k+1


gl_sqreg <- k
QMReg <- SQReg/gl_sqreg

gl_sqres <- n-p
QMRes <- SQRes/gl_sqres

gl_sqtotal <- n-1
```


```{r}
#calculando a estatistica F
(F_0 <- QMReg/QMRes)
```

```{r}
(QMTotal <- QMRes + QMReg)
```

Apresentamos os resultados obtidos na forma de tabela.

| **F.V.** | **G.L** | **S.Q.** | **Q.M.** | **F** |
|:---:|:---:|:---:|:---:|:---:|
| **Regressão** | $2$ | $9712.5$ | $4856.25$ | $97.05035$ |
| **Resíduo** | $13$ | $650.5$ | $50.03846$ |  |
| **Total** | $15$ | $10363$ |  $4906.288$  |  |
Table: Tabela ANOVA para o modelo ajustado.


Como nosso estimador $F \sim F(k,\ n - k - 1)$, podemos obter os quantis com o auxílio do R:

```{r}
alpha <- 0.03
(RR <- qf(alpha, df1 = k, df2 = n - k -1, lower.tail = F) )
```

Rejeitamos $H_0$ se $F_0 > F_{crit}$, sendo $F_{crit}$ o quantil teorico da distribuicao F com $k$ e $n-p$ graus de liberdade.

```{r}

if(RR < F_0){
  cat("Rejeita-se H0")
}
```

Dessa forma, podemos concluir com 97% de confiança que pelo menos uma das variáveis contribui significativamente ao modelo.

## 5. $R^2$ e $R^2 ajustado$

Agora que sabemos que nosso modelo de regressão linear múltipla é adequado, estamos interessados em medir quais covariavéis têm maior contribuição em resumir a variabilidade de nossa amostra.

Uma das formas de fazer isso, é utilizando o *coeficiente de determinação*, também conhecido como $R^2$, definido como:

$$ R^2 =  \frac{SQ_{Reg}}{SQ_{Total}}$$

$R^2$ é útil para quantificar o percentual da variabilidade dos dados explicado pelo modelo, contudo, como ele tende a aumentar conforme se adicionam mais covariáveis, sendo elas explicativas ou não. Uma forma de contornar esse impasse é através da utilização do $R^2$ ajustado:

$$R^2_{adj} = 1-  \frac{QM_{Res}}{QM_{Total}}$$

Para evitar duplicar o código utilizado para ajuste dos modelos, criamos uma função com este intuito:

```{r}
lm.anova <- function(x,y){
  x<- as.matrix(x)
  y<- as.matrix(y)

  n <- length(y)
  k <- ncol(x) # covariaveis utilizadas
  p <- k+1

  X <- matrix(c(rep(1,n), x), ncol = p, nrow = n, byrow = FALSE)
  Y <- matrix(y, ncol = 1, nrow = n)

  betas <- solve(t(X)%*%X)%*%t(X)%*%Y

  Y_est <- X%*%betas

  res <- Y - Y_est

  SQRes <- t(Y-Y_est)%*%(Y-Y_est)

  u <- c(rep(1,n))
  SQTotal <- t(Y)%*%Y - ((t(u)%*%Y)^2)/n

  SQReg <- SQTotal - SQRes

<<<<<<< HEAD

  SQReg <- t(betas)%*%t(X)%*%Y-((t(u)%*%Y)^2)/n
  gl_sqreg <- k
  QMReg <- SQReg/gl_sqreg

  SQRes <- t(Y-Y_est)%*%(Y-Y_est)
  gl_sqres <- n-p
  QMRes <- SQRes/gl_sqres

  SQTotal <- t(Y)%*%Y - ((t(u)%*%Y)^2)/n
  gl_sqtotal <- n-1


  #calculando a estatistica F
=======
  gl_sqreg <- k 
  QMReg <- SQReg/gl_sqreg 
  gl_sqres <- n-p 
  QMRes <- SQRes/gl_sqres 
  SQTotal <- t(Y)%*%Y - ((t(u)%*%Y)^2)/n 
  gl_sqtotal <- n-1
   
  #calculando a estatistica F 
>>>>>>> 383271d1c1467286d90fea2b1b7d9449315b7777
  F_0 <- QMReg/QMRes

  # calculo de R2
  R2 <-  SQReg/SQTotal

  QMTotal <- QMRes + QMReg
  #R2 ajustado
  R2adj <- 1 - QMRes/QMTotal
  #retorna uma lista com objetos relevantes ao modelo
  list(R2 = R2, R2adj = R2adj, betas = betas, Y_est = Y_est,
       anov = list(SQreg = SQReg, SQRes = SQRes, QMreg = QMReg))
<<<<<<< HEAD

=======
>>>>>>> 383271d1c1467286d90fea2b1b7d9449315b7777
}
```

Vamos criar uma lista com os cinco atributos presentes em nosso conjunto de dados, e ajustaremos modelos incrementalmente com as seguintes variáveis:

```{r}
feat <- c("Tamanho", "Temp", "Pressao", "FluxoCO2", "Umidade")
stepmodel <- 1:5 %>% map(~head(feat,.x))
```
```{r}
# Calcula R2 e R2adj para os cinco modelos
r2 <- stepmodel %>% map_dfr(~lm.anova(x = dados %>%
                                            select(all_of(.x)),
                                      y = dados$y)[c("R2adj", "R2")]) %>%
  cbind(x = 1:5)
```


```{r}
# gráfico de R2 e R2adj
r2 %>%
  pivot_longer(cols = !x) %>%
  ggplot(aes(x = x, y = value, color = name, label =  round(value, 3))) +
  geom_line()  +
  labs(title = "Variabilidade explicada pelos modelos de Regressão Linear Múltipla",
       x = "Número de covariáveis",
       y = "",
       color = "Métrica") +
  scale_color_brewer(palette = "Dark2",labels = c("R²", "R² ajustado")) +
  geom_label(show.legend = FALSE) +
  theme_pubclean()

```


Diferentemente do esperado, o valor de $R^2$ não aumentou com a inclusão de mais variáveis, e o valor de $R^2_{adj}$ apenas diminuiu após a inclusão da terceira variável. Temos aqui fortes indícios que nem todas as variáveis são significativas ao modelo.

O modelo que foi capaz de melhor explicar a fonte de variabilidade dos dados de acordo com o critério de maior $R^2_{adj}$ foi o modelo com apenas duas covariáveis, que apresentou $R^2_{adj} = 0.986$.

## 6. Testes nos Coeficientes de Regressão Individualmente

Usamos o teste t parcial, para saber se realmente todas as covariáveis são diferentes de zero. O teste, testa esta cada beta, mede a contribuição de uma determinada covariável considerando que outras co-variáveis estão no modelo.E, é definido como:
$H_0:\beta_j=0$ VS $H_1:\beta_j\neq 0$
Como consequência, Se $H_0$ for rejeitado, $X_i$ é importante dado que as outras covariáveis estão no modelo também.

Vale lembrar, ainda, que este é um teste marginal ou parcial, pois como dito anteriormente $\hat{\beta}_{i-1}$ depende de todas as outras covariáveis.

### Sendo:
```{r}
alpha <- 0.03
beta0_est <- betas[1,1]
beta1_est <- betas[2,1]
beta2_est <- betas[3,1]
beta3_est <- betas[4,1]
beta4_est <- betas[5,1]
beta5_est <- betas[6,1]
```

```{r}
t1 <- qt(alpha/2, n-p)
t2 <- qt(1-alpha/2, n-p)
cbind(t1,t2)
```

###       t1       t2
### [1,]  -2.4607 2.4607

Rejeitamos $H_0$ se $t<t_{(\alpha/2;n-p)}$ ou $t>t_{(1-\alpha/2;n-p)}$.

Testando para $\beta1$  

```{r}
t_b1 <- beta1_est / (sqrt( QMRes* solve(t(X)%*%X)[2,2] ) )
t_b1
```
### [,1]
### [1,] 2.037315

```{r}
if(t_b1 < t1 || t_b1>t2){
 cat("Rejeita-se H0")
}
```
### Não rejeita-se H0

Testando para $\beta2$
```{r}
t_b2 <- beta2_est / (sqrt( QMRes* solve(t(X)%*%X)[3,3] ) )
t_b2
```
### [,1]
### [1,] 5.36493

```{r}
if(t_b2 < t1 || t_b2>t2){
 cat("Rejeita-se H0")
}
```
### Rejeita-se H0

Testando para $\beta3$
```{r}
t_b3 <- beta3_est / (sqrt( QMRes* solve(t(X)%*%X)[4,4] ) )
t_b3
```
### [,1]
### [1,] 0.3395525

```{r}
if(t_b3 < t1 || t_b3>t2){
 cat("Rejeita-se H0")
}
```
### Não rejeita-se H0

Testando para $\beta4$
```{r}
t_b4 <- beta4_est / (sqrt( QMRes* solve(t(X)%*%X)[5,5] ) )
t_b4
```
### [,1]
### [1,] 3.618999e-14

```{r}
if(t_b4 < t1 || t_b4>t2){
 cat("Rejeita-se H0")
}
```
### Não rejeita-se H0

Testando para $\beta5$
```{r}
t_b5 <- beta5_est / (sqrt( QMRes* solve(t(X)%*%X)[6,6] ) )
t_b5
```
### [,1]
### [1,] -12.08807
```{r}
if(t_b5 < t1 || t_b5>t2){
 cat("Rejeita-se H0")
}
```
### Rejeita-se H0

Portanto, seguindo a regra apresentada no início da questão, concluímos que $\beta2$ e $\beta5$ rejeitam $H_0$. Logo, Temperatura e  Tamanho são covariáveis significativas para o modelo, dado que já temos pelo menos uma variável significativa no modelo conforme constatado no teste f anteriormente.

## 7.  Teste um Subconjunto de Coeficientes
* Faça análise e Escolha do Melhor Modelo, Qual modelo seu grupo acredita ser o melhor? Apresente omodelo escolhido pelo grupo (Equação do modelo ajustado, com as estimativas dos $\beta$'s e $\sigma^2$ e interpretação do modelo). Justifique a escolha das co-variáveis. Faça uso do teste de um subconjuntode coeficientes nesta questão ilustrado na aula 14 e 15.

Equação de melhor modelo: $Y = \beta2x^2 + \beta5x^5$

Dando início as interpretações,consideremos um exemplo com $\beta2$. Sabemos, que se todo resto da equação fica constante o $\beta2$ representa o incremento de y dado a variação de uma unidade em $x_1$, com $x_y$ diferente de i mantido com todas as constantes.

Estimando os Betas para o melhor modelo
```{r}
x <- dados%>%select(Temp, Tamanho)
y <- dados$y
modelo <- lm.anova(x,y)
modelo$betas
```
###             [,1]
### [1,]  80.1346055
### [2,]   0.2821429
### [3,] -16.0649819

Estimando $\sigma2$ para o melhor modelo
```{r}
modelo$anov$SQRes/13
```
###          [,1]
### [1,] 67.82692

Realizando o teste de um subconjunto de coeficientes
```{r}
F_testeparcial <- modelo$anov$F_0
```

```{r}
alpha <- 0.03
gl_modredu <- 2
RR <- qf(alpha, df1 = gl_modredu, df2 = n - gl_modredu -1, lower.tail = F)
RR
```
### 4.648139

Seguindo a regra, se $F>F_\text{Tabelado}$ rejeitamos $H_0$.

```{r}
if(RR < F_testeparcial){
    cat("Rejeita-se H0")
}
```
### Reita-se H0

Como foi rejeitada $H_0$ temos que o subconjunto de coeficientes testados (variáveis Temperatura e Tamanho) contribui significativamente para o modelo.Vale ressaltar que este teste é análogo ao teste feito anteriormente usando a estatística t.


## 8. IC Para Coeficientes de Regressão

Faça o intervalo de confiança para os coeficientes de regressão de seu modelo. Apresente conclusões ou interpretação para cada um deles.

Dado que o melhor modelo que encontramos foi aquele com as variáveis, **Temperatura (Temp)** e **Tamanho**, estimaremos o intervalo de confiança para os coeficientes $\beta_j = \beta_2,\beta_5$ com um coeficiente de confiança de $97\%$

Definimos o Índice de confiança $\gamma = (100 - \alpha)\%$, também chamado de *coeficiente de confiança*

```{r}
#Definindo o alpha e o t crítico
alpha <- 0.03
#para o melhor modelo temos que k =2
k <- 2
p <- k + 1
t1 <- qt(alpha/2, n-p)
t2 <- qt(1-alpha/2, n-p)
```

Para $\beta_2$ temos que:

$97\%; \hat{\beta}_2=0,2821; QM_{Res}=65,05; (X^T X)_{33}^{-1}=5,1020e^-5; T_{(0,975;22)}=2,4358$

```{r}
#Testando para o beta2
dp_b2 <- sqrt(sigma2 * diag(C_jj)[3])
b2_lim_inf <- betas[3] - t2*dp_b2
b2_lim_sup <- betas[3] - t1*dp_b2
IC_b2 <- cbind(b2_lim_inf, b2_lim_sup)
```

Para $\beta_5$ temos que:

$97\%; \hat{\beta}_5=-16,065; QM_{Res}=65,05; (X^T X)_{66}^{-1}=0,0326; T_{(0,975;22)}=2,4358$

```{r}
#Testando para beta5
dp_b5 <- sqrt(sigma2 * diag(C_jj)[6])
b5_lim_inf <- betas[6] - t2*dp_b5
b5_lim_sup <- betas[6] - t1*dp_b5
IC_b5 <- cbind(b5_lim_inf, b5_lim_sup)
```

```{r}
#Estruturando em uma tabela
IC_tab <- rbind(IC_b2, IC_b5)
row.names(IC_tab) <- c("beta2", "beta5")
colnames(IC_tab) <- c("IC (0.015)", "IC (0.985)")
IC_tab %>% kable(caption = "Intervalos de Confiança para os coeficientes do modelo")
```
A tabela acima pode ser interpretada da seguinte maneira: Temos 97% de confiança de que o real valor do parâmetro $\beta_j$ está contido nos intervalos acima. Outro interpretação seria, se realizássemos esse experimento diversas vezes, concluiríamos que em 97% das vezes o real valor de $\beta_j$ estaria dentro do intervalo.


## 9. IC para E(Y)
Podemos também construir intervalos de confiança para pontos específicos do nosso conjunto de dados.
Seja $\boldsymbol{x}_0=[1,x_{01},x_{02},\ldots,x_{0k}]$, podemos obter uma estimativa de $\hat{y}$ a partir da seguinte equação:

$$\hat{y_0}=x_0^{T}\hat{\beta}$$
Em seguida, estimamos a variância, ou erro padrão, de $\hat{y_0}$ através da equação:

$$Var(\hat{y_0})=\hat{\sigma}^2x_0^T(X^TX)^{-1}x_0$$
Desse modo, construímos nosso intervalo de confiança para a média de $y$ no
ponto $x_0$, conforme a seguinte equação:

$$\hat{y_0} - t_{\alpha/2, n-p}\sqrt{\hat{\sigma}^2x_0^{T}(X^TX)^{-1}x_0}\leq E(y|x_0)\leq \hat{y_0} + t_{\alpha/2, n-p}\sqrt{\hat{\sigma}^2x_0^{T}(X^TX)^{-1}x_0}$$
A seguir, calculamos o Intervalo de Confiança (IC) para a média , dado um $x_0$ - ou seja, dado uma observação do nosso conjunto de dados. De início, iremos calcular para o modelo com todas as covariáveis e, em seguida, para o modelo com as covariáveis mais significativas. A fim de comparar os diferentes modelos e seus IC , utilizaremos como métrica de avaliação o **Erro Quadrático Médio (MSE)** e o **Coeficiente de Determinação $R^2$ Ajustado**

$$MSE = \sum_{i=1}^n\frac{(\hat{y_i} - y_i)^2}{n}$$
$$R^2_{aj} = 1 -\frac{SQ_{Res}/(n-p)}{SQ_{T}/(n-1)}$$

### IC de E(Y), considerando todas as covariáveis no modelo

```{r}

sigma2 <- SQRes/(n-p)
t3 <- qt(1-alpha/2, n-p)



mean_IC <- data.frame(low_b = numeric(0), upper_b = numeric(0))
y_pred <- data.frame(value = numeric(0))

for (row in 1:nrow(X)){
  x0 <- X[row,]
  y0 <- x0%*%betas
  se <- sqrt(sigma2%*%t(x0)%*%C_jj%*%x0)
  low_b <- y0 - t3*se
  upper_b <- y0 + t3*se
  tab <- cbind(low_b, upper_b)
  mean_IC <- rbind(mean_IC, tab)
  y_pred <- rbind(y_pred, y0)
}

colnames(mean_IC) <- c("Lower Bound", "Upper Bound")
len_mean_IC <- mean_IC$`Upper Bound` - mean_IC$`Lower Bound`
mean_IC <- cbind(mean_IC, len_mean_IC)
mean_IC <- cbind(mean_IC, y_pred)
mean_IC <- cbind(mean_IC, dados$y)

colnames(mean_IC) <- c("Lower Bound", "Upper Bound", "Interval Length","Predicted Value", "Real Value")

error <- abs(mean_IC$`Predicted Value` - mean_IC$`Real Value`)
mean_IC <- cbind(mean_IC, error)

colnames(mean_IC) <- c("Lower Bound", "Upper Bound", "Interval Length","Predicted Value", "Real Value", "Absolute Error")

mse <- mean(mean_IC$`Absolute Error`**2)

mean_IC %>% kable(caption = "**Estimação do IC de E(Y) para cada observação do conjunto de dados considerando todas as covariáveis** (MSE = 40.65, R^2 = 0.90)")
```

O resultado da tabela acima nos diz que, dado qualquer observação do nosso conjunto dados, podemos afirmar com 97% de confiança que o verdadeiro valor de $y = rendimento~de~azeite~por~lote~de~amendoim|x_0$ está contido neste intervalo, considerando que o nosso modelo leva em conta todas as covariáveis $x_i$.

Entretanto, sabemos que o melhor modelo encontrado foi aquele com as covariáveis **Pressão, Temperatura** e **Tamanho**. Considerando apenas essas variáveis explicatórias, obteremos os seguintes intervalos de confiança:

```{r}
#O melhor modelo teve os valores dos betas modificados
#Redefinindo a matrix X
X_2 <- X[,c(1,3,6)]
#Redefinindo a matrix C_jj
C_jj2 <- solve(t(X_2)%*%X_2)

betas2 <- solve(t(X_2)%*%X_2)%*%t(X_2)%*%Y
betas2
```


```{r}
mean_IC2 <- data.frame(low_b = numeric(0), upper_b = numeric(0))
y_pred2 <- data.frame(value = numeric(0))

#recalculando a estatística teste t
#agora nosso k =4
t4 <- qt(1-alpha/2, n-3)
sigma <- SQRes/(n-3)


for (row in 1:nrow(X)){
  x0 <- X[row,c(1,3,6)]
  y0 <- x0%*%t(t(betas2))
  se <- sqrt(sigma2%*%t(x0)%*%C_jj2%*%x0)
  low_b <- y0 - t4*se
  upper_b <- y0 + t4*se
  tab <- cbind(low_b, upper_b)
  mean_IC2 <- rbind(mean_IC2, tab)
  y_pred2 <- rbind(y_pred2, y0)
}

colnames(mean_IC2) <- c("Lower Bound", "Upper Bound")
len_mean_IC2 <- mean_IC2$`Upper Bound` - mean_IC2$`Lower Bound`
mean_IC2 <- cbind(mean_IC2, len_mean_IC2)
mean_IC2 <- cbind(mean_IC2, y_pred2)
mean_IC2 <- cbind(mean_IC2, dados$y)

colnames(mean_IC2) <- c("Lower Bound", "Upper Bound", "Interval Length","Predicted Value", "Real Value")

error <- abs(mean_IC2$`Predicted Value` - mean_IC2$`Real Value`)
mean_IC2 <- cbind(mean_IC2, error)

colnames(mean_IC2) <- c("Lower Bound", "Upper Bound", "Interval Length","Predicted Value", "Real Value", "Absolute Error")

mse2 <- mean(mean_IC2$`Absolute Error`**2)

mean_IC2 %>% kable(caption = "**Estimação do IC para cada observação do conjunto de dados considerando apenas as covariáveis significativas** (MSE = 55.10, R^2 = 0.985)")
```

Do mesmo modo que a tabela anterior, podemos afirmar com 97% de confiança que o verdadeiro valor de $y = rendimento~de~azeite~por~lote~de~amendoim|x_0$ está contido neste intervalo, dado que o nosso modelo possui as covariáveis mais significativas.

Vale notar que é sempre preferível ter intervalos de confiança menores, uma vez que eles nos dão uma melhor ideia da magnitude da variável dependente $y$. Porém, nosso melhor modelo apresentou intervalos de confiança muito maiores que o modelo completo. Isso ocorre pois o modelo completo apresenta uma **erro padrão**, tal que: $Var(\hat{y_0})=\hat{\sigma}^2x_0^T(X^TX)^{-1}x_0 \approx 3.063$ enquanto que o melhor modelo, com apenas 3 covariáveis, apresenta um **erro padrão**: $Var(\hat{y_0})=\hat{\sigma}^2x_0^T(X^TX)^{-1}x_0 \approx 4.33$. Essa diferença é responsável pelo aumento do comprimento do IC. Ainda sim, vale lembrar que o $R^2_{aj}$ do melhor modelo é maior que o do modelo completo, o que indica que esse modelo é, de fato, melhor.

Esse fato ilustra um conceito interessante em modelos preditivos e aprendizado de máquina: **o dilema viés-variância**. Esse dilema estabelece que a diminuição da variãncia de um modelo sempre ocorre às custas do aumento do viés e, vice-versa.

Podemos definir **Viés** como a diferença entre o valor real e o predito. Para múltiplas observações, computamos a média desta diferença:
$$Viés = E(\hat{y_i} - y_i)$$
E a **Variância** como a média dos desvios quadrados dos valores preditos $\hat{y_i}$ em relação à média dos valores preditos $E(\hat{y_i})$ :
$$Var(\hat{y_i}) = \frac{\sum_{i=1}^n(\hat{y_i} - E(\hat{y_i}))^2}{n}$$
Pode-se mostrar que o MSE pode ser decomposto de forma a gerar a seguinte equação:

$$MSE = Viés^2+Variância+Ruído$$

```{r, echo=FALSE}
#Viés do modelo completo
bias_fullm <- abs(mean(mean_IC$`Predicted Value` - mean_IC$`Real Value`))

#Viés do melhor modelo
bias_bestm <- abs(mean(mean_IC2$`Predicted Value` - mean_IC2$`Real Value`))

#Variancia do modelo completo
var_fullm <- var(mean_IC$`Predicted Value` - mean_IC$`Real Value`)

#Variancia do melhor modelo
var_bestm <- var(mean_IC2$`Predicted Value` - mean_IC2$`Real Value`)

#criando uma tabela com os valores
vies <- c(round(bias_bestm,2), round(bias_fullm,2))
variancia <- c(round(var_bestm,2), round(var_fullm,2))

bias_var_m <- data.frame(Viés = vies,
                         Variância = variancia)

row.names(bias_var_m) <- c("Melhor modelo", "Modelo completo")
bias_var_m %>% kable(caption = "Tabela Viés X Variância")
```
Percebemos na tabela acima que ambos os modelos apresentaram $Viés = 0$, o que provavelmente ocorreu devido ao pequeno núemro de observações, mas ainda sim indica que nosso modelo não está enviesado a um modelo em particular. Vale notar também que o **melhor modelo**, mesmo apresentando maior variância é capaz de explicar - a partir do $R_{aj}$ - $98\%$ da variabilidade, enquanto que o **modelo completo**, explicava cerca de $90\%$, apresentando uma variabilidade bem menor, fato esse que, dado esse conjunto de dados, seria um indicativo de *sobreajuste*.
