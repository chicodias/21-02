---
title: "SME0820 - Modelos de Regressão e Aprendizado Supervisionado I - Trabalho I"
author:
  - Brenda da Silva Muniz 11811603
  - Francisco Rosa Dias de Miranda 4402962
  - Heitor Carvalho Pinheiro 11833351
  - Mônica Amaral Novelli 11810453
date: "Setembro 2021"
output: pdf_document
---

Neste trabalho, nosso objetivo é ajustar um modelo de regressão linear simples ao conjunto de dados fornecido, utilizando linguagem R. Para esta tarefa, descreveremos cada etapa de nosso *pipeline*.

O dataset B.3 contém dados sobre o rendimento de Gasolina, em milhas, de 32 automóveis diferentes. Ajuste o modelo de regressão linear simples que relaciona o rendimento da gasolina (y) (Milhas por litro) e a cilindrada do motor (x1) (polegadas cúbicas). Use sempre Significância: 99%.

Primeiramente, vamos carregar os módulos utilizados nesta análise. Caso não possua algum dos pacotes, utilize o comando `install_packages("Nome_do_pacote")`.


```{r pkgs, warning=FALSE,message = FALSE}
library(tidyverse)
library(ggpubr)
library(corrplot)
library(DataExplorer)
library(GGally)
library(knitr)
library(data.table)
```

Com os pacotes carregados em nosso ambiente, lemos o arquivo `.csv` disponibilizado colocando-o na mesma pasta de nosso projeto. Vamos inspecionar o que foi carregado com auxílio do comando `head()`, que exibe as 5 primeiras observações.

```{r read-load, warning=FALSE,message = FALSE}
dados <- read_csv("data-table-B3.csv", locale = locale(decimal_mark = ","))
```

```{r}
head(dados) %>% kable(caption = "Cinco primeiras observações do dataset")
```
Vamos separar nossa base em treino e teste, onde guardaremos 4 observações para realizar a previsão mais tarde.

```{r}
set.seed(42)
smp <- sample(32, 4)
treino <- dados[-smp,] %>% select(y, x1)
teste <- dados[smp,] %>% select(y, x1)
y <- treino$y
x1 <- treino$x1
n <- length(y)
```

## Parte **a)**:
  - Descrição do banco de dados
  - Definição das variáveis
  - Análise exploratória inicial

```{r summary}
summary(dados %>% select(y,x1)) %>% kable(caption = "Sumário das variáveis utilizadas")
```

  Com o comando **summary** verificamos as principais medidas descritivas para cada variável (feature) presente no nosso conjunto de dados. Temos 12 features e ajustaremos o modelo com base na feature x1.

  **Dimensão dos dados**

```{r fig.cap= "Dimensão dos Dados"}
dim(dados) %>% kable(caption = "Dimensão dos dados")
```
## Análise Exploratória Básica

```{r fig.cap="Gráfico de dispersão para as variáveis X1 e Y"}
ggplot(dados, aes(x=dados$x1, y = dados$y)) + geom_point() + #geom_smooth(method = "lm") +
  ggtitle("Cilíndradas Vs Rendimento") + xlab("cc/pol.c") + ylab("Rendimento (mi / L)") +
  theme_pubclean() +
  theme(plot.title = element_text(size = 20, hjust = .5))
```
A partir do gráfico de dispersão acima parece que existe uma relação linear entre as variáveis $X_1$ e $Y$. Vamos verificar tal relação a partir do coeficiente de Pearson $\rho$.

**Teste de Correlação de Pearson entre as variáveis $X_1$ e $Y$**

```{r fig.cap="Coeficiente de Pearson para as variáveis $X_1$ e $Y$"}
cor.test(dados$x1, dados$y)
```

O teste de correlação de Person resultou em um $\rho = -~0.87$ o que indica uma forte correlação entre as variáveis. Nosso objetivo é estimar a influência de $X_1$ sobre $Y$, ou seja, como $Y$ varia em relação às variações em $X_1$.

```{r fig.cap="Boxplot do regressor $X_1$"}
dados %>%
  ggplot(aes(x = "", y = dados$x1)) +
    geom_boxplot(color = "blue") +
    stat_summary(fun = mean, geom = "point", shape = 20, size = 5, color = "red", fill = "red") +
    geom_jitter(color="black", size=1, alpha=.9) +
    theme(plot.title = element_text(size = 15, hjust = .5)) +
    ggtitle("Boxplot da Variável (X1)") +
    xlab("") + ylab("Cilíndradas por Polegada Cúbica")
```
Podemos perceber a partir do boxplot acima e da função summary que $50\%$ dos carros tem menos de $318$ cilíndradas.

```{r fig.cap= "Boxplot da variável resposta $Y$"}
dados %>%
  ggplot(aes(x = "", y = dados$y)) +
    geom_boxplot(color = "blue") +
    stat_summary(fun = mean, geom = "point", shape = 20, size = 5, color = "red", fill = "red") +
    geom_jitter(color="black", size=1, alpha=.9) +
    theme(plot.title = element_text(size = 15, hjust = .5)) +
    ggtitle("Boxplot da Variável (Y)") +
    xlab("") + ylab("Rendimento por motor em milhas por litro")
```
Pelo boxplot acima percebemos que os dados da variável y parecem seguir uma distribuição Normal - o que será confirmado com a análise de um histograma. Ainda, destaca-se quatro outliers, cujo rendimento é superior a $30~mi/L$.

```{r fig.cap = "Histograma do Rendimento do Motor"}
dados %>%
  ggplot(aes( x=dados$y)) +
    geom_histogram(aes(y=..density..), color = "black", fill = "lightblue", bins = 13) + # xlab("Rendimento") + ylab("Frequência") +
    geom_vline(aes(xintercept=mean(dados$y), color = "Média"), linetype = "solid") +
    geom_vline(aes(xintercept=median(dados$y), color = "Mediana"), linetype = "solid") +
    labs(title = "Histograma da variável Y ( em milhas por L)") +
    scale_x_continuous("Rendimento", breaks = seq(10,38,2)) +
    scale_y_continuous("Frequência Relativa") +
    scale_color_manual(name=" ", values = c("red", "black")) +
    ylab("Frequência") +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme_pubclean()
```
O histograma acima representa a distribuição da variável y. Fica claro, portanto, que os dados da variável resposta se assemelham a uma distribuição normal, não fossem os outliers.


```{r}
dados %>%
  select(y,x1) %>%
  filter(y>30) %>%
  kable(caption = "Tabela com os 4 outliers presentes nos valores de Y")
```
A partir do histograma e da tabela acima, concluimos que os 4 outliers referem-se aos valores: $30.4$, $31.9$, $34.7$, e $36.5$.


Vamos verificar a distribuição da variável $X_1$

```{r fig.cap= "Histograma das cilíndradas por polegada cúbica"}
dados %>%
  ggplot(aes( x=dados$x1)) +
    geom_histogram(aes(y=..density..), color = "black", fill = "lightgreen", bins = 15) + # xlab("Rendimento") + ylab("Frequência") +
    geom_vline(aes(xintercept=mean(dados$x1), color = "Média"), linetype = "solid") +
    geom_vline(aes(xintercept=median(dados$x1), color = "Mediana"), linetype = "solid") +
    labs(title = "Histograma da variável X1 (Cilindradas por Polegada Cúbica)") +
    scale_x_continuous("Cilindradas", breaks = c(100,150,200,250,300,350,400,450,500)) +
    scale_y_continuous("Frequência relativa") +
  scale_color_manual(name=" ", values = c("red", "black"))+
  #limits = c(85,500,1)) +
    ylab("Frequência") +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme_pubclean()
```
O histograma da variável $X_1$ não apresenta uma distribuição bem definida, entretanto, destaca-se maior frequência para os valores em torno de $350$ cilíndradas.

**Moda dos valores de $X_1$**
```{r fig.cap="Valor da moda de $X_1$"}
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

mode_x1 = getmode(dados$x1)
print(mode_x1)
```
De fato, a moda para os valores de $X_1$ é $350$.


## Parte **b)**:

Consultar e descrever brevemente os conceitos Data splitting, cross validation, overfitting, underfitting, missing data, encoding data.

1. **Data Splitting**: Data Splitting ou também “divisão de dados” é uma abordagem para proteger dados confidenciais de acesso não autorizado, criptografando os dados e armazenando diferentes partes de um arquivo em servidores diferentes. Quando os dados divididos são acessados, as partes são recuperadas, combinadas e descriptografadas.

2. **Cross Validation**: Cross Validation ou também “validação cruzada” é uma técnica muito utilizada para avaliar o desempenho de modelos de aprendizado de máquina. Consiste, basicamente, em particionar os dados em conjuntos, onde um conjunto é utilizado para treino e outro para teste e avaliação do desempenho do modelo. A utilização correta da técnica tem altas chances de detectar se um modelo está sobreajustado aos seus dados de treinamento, ou seja, sofrendo overfitting. Vale ressaltar que existem vários métodos de aplicação da validação cruzada.

3. **Overfitting**: Overfitting ou também "Sobreajuste" consiste na situação em que o modelo se ajusta bem demais ao conjunto de treinamento. Ou seja, nos dados de treinamento, em geral, a acurácia do modelo é muito alta (e, quando há 100% de acurácia dizemos que o modelo "memorizou" os dados). Isso ocorre pois além de aprender os detalhes dos dados o modelo também aprende os ruídos, o que prejudica sua capacidade de generalização no conjunto de teste. Em geral, quanto maior a complexidade do modelo mais propenso ao Overfitting ele se torna.

4. **Underfitting**: Já o Underfitting, por outro lado, refere-se ao problema em que o modelo não é capaz de modelar o conjunto de treinamento e nem generalizar para dados nunca vistos. Em geral, a solução reside no aumento da complexidade do modelo ou a troca do algoritmo.

5. **Missing data**: Missing data, muitas vezes referido como missing values (com tradução literal: valores que faltam), é um conceito utilizado para quando alguma(s) observação(ões) no conjunto de dados está(ão) vazia(s), causando ambuiguidade e falta de precisão para a análise do mesmo.
Na análise multivariada, temos uma relação proporcional da quantidade de variáveis a serem relacionadas com a falta de rigor causada pelos missing values.

6. **Encoding data**: Encoding data (de tradução literal: dados codificados) é o nome dado para o processo de converter dados para um formato específico, assegurando sua transmissão e otimizando o modelo. Seu processo inverso - ou seja, a decodificação - refere-se a extrair as informações da forma convertida.

## Parte **c)**:

  1. Calcular $S_{XX}$,$S_{YY}$ e $S_{XY}$


Calculando o valor de $S_{xx}$

$$S_{XX} =  \sum_{i=1}^n (x -\bar{x})^2$$

```{r}
xbarra=mean(x1)
x1-xbarra
(x1-xbarra)^2
Sxx=sum((x1-xbarra)^2)
```


### Calculando o valor de $S_{yy}$

$$S_{YY} =  \sum_{i=1}^n (y - \bar{y})^2$$

```{r}
ybarra=mean(y)
y-ybarra
(y-ybarra)^2
Syy=sum((y-ybarra)^2)
```

### Calculando o valor de $S_{xy}$

$$S_{XY} =  \sum_{i=1}^n (x - \bar{x})(y - \bar{y})$$

```{r}
Sxy=sum((x1-xbarra)*(y-ybarra))
cbind(Sxx,Syy,Sxy)
```

  2. Ajustar um modelo de regressão linear simples, apresentar a estimativa de $\beta_0, \beta_1$ e $\sigma^2$ e fazer um gráfico com a reta ajustada

## Estimacao dos parametros

$$\beta_1 = S_{XY}/S_{XX}$$

### Calculando o valor do coeficiente angular $\beta_1$

```{r}
b1_est <- Sxy/Sxx
```

### Calculando o valor do intercepto $\beta_0$

```{r}
b0_est <- mean(y) - b1_est*mean(x1)
```

### Calculando o estimador de $\sigma^2$ não viesado.

Tal estimador é obtido através da soma do quadrado dos resíduos, definido pela variável *QMres*, de modo que:

```{r}
# Soma do quadrado da regressão:
SQreg <- b1_est*Sxy
# Soma do quadrado total:
SQtotal <- sum((y-mean(y))^2)
# Diferença entre a soma do quadrado da regressão e a soma do quadrado total:
SQres <- SQtotal - SQreg
# Soma do quadrado dos resíduos:
QMres <- SQres/(n-2)
```
```{r}
dados %>% ggplot(aes(x= x1, y= y)) + geom_point() +
 geom_smooth(method='lm', formula= y~x) +
  theme_pubclean()
```

```{r}
#Arredondando b0 e b1
round(b0_est, 4)
round(b1_est, 4)
```

Consequentemente, a reta ajustada é:
$$\widehat{Y}_i=1224.043-0.7769X_i$$


  3. Calcule o valor dos $\hat{Y}$ e o valor dos resíduos para seu modelo, resumo e histograma dos resíduos, e faça uma análise da distribuição destes.

  O cálculo de $\hat{Y}$ pode ser realizado utilizando o modelo de regressão linear simples, em que a variabilidade de interesse é dada em função de uma única covariável - no caso, *x1*. No R, podemos expressar $\hat{Y}$ como sendo:

```{r}
y_pred <-  b0_est + b1_est*x1
```

Os resíduos se dão pelo desvio entre as observações e os valores preditos, sendo uma medida de variabilidade na variável resposta onde qualquer desvio relativo a suposição dos erros deveria aparecer. Analisá-los nos permite um discernimento maior em relação a quão adequado é o modelo. Fazendo uso do cálculo de *y_pred* feito anteriormente, salvamos nossos resíduos em uma variável *res* abaixo.

```{r}
res <- y - y_pred
```

Utilizando o comando *summary*, podemos observar as principais medidas descritivas da variável, o que nos auxilia para a análise da mesma.

```{r}
tibble(summary(res)) %>% kable(caption = "Principais medidas descritivas dos resíduos")
```

Também podemos construir um histograma, assim como um gráfico quantil-quantil, que facilitará a visualização do comportamento dos resíduos.

```{r}
# Histograma
n_res <- length(res)
normalDist<- rnorm(length(res), mean =  mean(res), sd= sd(res))
dat<- data.frame(error = res, norm = normalDist)
ggplot(tibble(res), aes(x = res)) +
  ylab("Frequência") +
  geom_histogram(aes(y=..density..), color = "black", fill = "#FF69B4", bins=12, position="identity")+
  labs(title = "Histograma dos resíduos")+
  geom_density(data = dat) +
  scale_x_continuous("Resíduos", limits = c(-8,8,1), breaks = c(-8:8))+
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_classic()
```

```{r qqplotresidu}
# Q-Q
qqnorm(res, main = "Q-Q Plot da Normal com os resíduos", ylab =" Quantis dos resíduos ", xlab = " Quantis da Normal ")
qqline(res, col = "#FF69B4")
```


Desse modo, é possível observar que os resíduos tendem a se aproximar cada vez mais da normal. Posteriormente, a normalidade dos resíduos será abordada com mais precisão.

  4. testes de hipotese para $\beta_0$ e $\beta_1$

Para realizarmos nossos testes de hipóteses, é necessário o estimador do parâmetro $\sigma^2$ do nosso modelo, uma vez que ele não é dado. Tal estimador não viesado é obtido através da soma do quadrado dos resíduos, definido pela variável *QMres*, calculada no item 3 de modo que:

```{r}
# Soma do quadrado da regressão:
SQreg <- b1_est*Sxy
# Soma do quadrado total:
SQtotal <- sum((y-mean(y))^2)
# Diferença entre a soma do quadrado da regressão e a soma do quadrado total:
SQres <- SQtotal - SQreg
# Soma do quadrado dos resíduos:
QMres <- SQres/(n-2)
```

A partir disso, podemos prosseguir com nossos testes de hipóteses para $\beta_1$ e $\beta_0$, com decisão de rejeitar ou não $H_0$, uma vez que este representa o parâmetro se igualar a 0 estatisticamente caso não seja rejeitado, descrevendo a significância da contribuição do mesmo.

  * Testagem se $\beta_1 = 0$:

$\beta_1$ possuí distribuição Normal com média $\beta_1$ e variância $\sigma^2/S_{xx}$, com isso, definimos:

```{r}
dp_b1 <- (sqrt(QMres/Sxx))
t0_b1 <- b1_est/dp_b1
```

  Pelo enunciado, é dado que $\alpha= 5\%$. Se H0 não for rejeitado, temos que $\beta_1$ é estatisticamente igual a zero.
  A partir disso, definimos $\alpha$ e dois quantis, de modo que *t1* é o quantil $\frac{\alpha}{2}$ da distribuicao $t$ com grau de liberdade $ n -2$, enquanto *t2* é o quantil $\frac{1-\alpha}{2}$ da distribuicao $t$ com grau de liberdade $n -2$.
  Com esses dados, podemos construir nosso programa de decisão que retornará caso $H_0$ seja rejeitado.

```{r}
alpha <- 0.05
t1 <- qt(alpha/2,n-2)
t2 <- qt(1-alpha/2,n-2)
if(t0_b1 < t1 || t0_b1>t2){
  cat("Rejeita-se H0")
}
```
Para $\alpha= 1\%$:

```{r}
alpha <- 0.01
t1 <- qt(alpha/2,n-2)
t2 <- qt(1-alpha/2,n-2)
if(t0_b1 < t1 || t0_b1>t2){
  cat("Rejeita-se H0")
}
```


Realizando o teste, temos que H0 é rejeitado, logo, $\beta_1$ é diferente de zero.

  * Testagem se $\beta_0 = 0$:

  $\beta_0$ possuí distribuição Normal com média $\beta_0$ e variância $\sigma^2((\frac{1}{n})+\frac{\bar{X}}{S_{xx}}))$, com isso, definimos:

```{r}
dp_b0 <- (sqrt( QMres *( (1/n) + (mean(x1))^2/Sxx )))
t0_b0 <- b0_est/dp_b0
```

  Em um processo semelhante à testagem de $\beta_1$, com $\alpha$= 5% e os mesmos quantis, também é possível a construção de nossa função de decisão. Se H0 não for rejeitado, temos que $\beta_0$ é estatisticamente igual a zero.

```{r}
alpha <- 0.05
t1 <- qt(alpha/2,n-2)
t2 <- qt(1-alpha/2,n-2)
if(t0_b1 < t1 || t0_b1>t2){
  cat("Rejeita-se H0")
}
```
Para $\alpha= 0.01$

```{r}
alpha <- 0.01
t1 <- qt(alpha/2,n-2)
t2 <- qt(1-alpha/2,n-2)
if(t0_b1 < t1 || t0_b1>t2){
  cat("Rejeita-se H0")
}
```

Realizando o teste, temos que H0 é rejeitado, logo, $\beta_0$ é diferente de zero.


 5. intervalos de confiança

## ***Intervalos de Confiança***

Intervalos de Confiança para $(\beta_0,\beta_1,\sigma^2)$ e $E(Y)$.

***Calculando intervalo de Confiança para $\beta_1$***
```{r icbeta1}
b1_min <- b1_est-t2*dp_b1
b1_max <- b1_est-t1*dp_b1
IC_b1_est <- cbind(b1_min, b1_max)
IC_b1_est
```


**Interpretação:** Cada incremento em polegada cúbica na cilindrada do motor
aumenta o consumo em milhas por litro em -0.0643, com uma margem de erro de aproximadamente 0.014 para mais ou para menos.

***Calculando intervalo de Confiança para $\beta_0$***
```{r icbeta0}
b0_min <- b0_est-t2*dp_b0
b0_max <- b0_est-t1*dp_b0
IC_b0_est <- cbind(b0_min, b0_max)
IC_b0_est
```



***Calculando intervalo de confiança para $\sigma^2$***

Lembrado que $SQres/\sigma^2$ tem Distribuição
qui-quadrado com $(n-2)$ G.L.
```{r icsigmaone}
t1_sig <- qchisq(alpha/2, n-2)
t2_sig <- qchisq(1-alpha/2,n-2)
```


```{r icsigmatwo}
sig_min <- SQres/t2_sig
sig_max <- SQres/t1_sig
```


```{r icsigmathree}
IC_sig_est <- cbind(sig_min, sig_max)
IC_sig_est
```

**Calculando intervalo de confiança para a esperança de y**

Aqui, calculamos o valor médio da variável resposta para para um valor particular da covariável $\mu(y|x_0)$.

**Lembrando:**

1. O valor médio da variável resposta é dado um $X_0$.
2. $\overline{Y}$ tem Distribuição Normal.
com média $\beta_0+\beta_1*\overline{X}$ e variância $\sigma^2/n$.
3. $\beta_1$ tem Distribuição Normal com média $\beta_1$ e variância
$\sigma^2/Sxx$.
4. A Esperança de $Y|X_0$ é Normal.
5. a Variância de $MIy|X_0$ é $\sigma^2*(1/n + ((X_0-\overline{X}^2)/Sxx)$,
 $t_1 =$ quantil da dist. $t(\alpha/2,n-2)$,
 $t_2 =$ quantil da dist. $t(1-\alpha/2,n-2)$,
 $\alpha = 0,05$.

Usaremos $X_0$ como sendo o proprio $\overline{X}$.
```{r x0exbarra}
X0 <- mean(x1) # poderia ser outro valor
v_medio <- (mean(y)+b1_est* (X0-mean(x1)) )
auxiliar <- sqrt(QMres*(1/n + (X0 - mean(x1)) /Sxx ))
```

**Intervalo De Confiança**
```{r icmedio}
v_medio_min <- v_medio - t2*auxiliar
v_medio_max <- v_medio - t1*auxiliar
IC_v_medio <- cbind(v_medio_min, v_medio_max)
IC_v_medio
```

## ***Intervalo de predição***

Se quisessemos predizer a mortalidade baseado em um novo valor
da variavel explicativa utilizada. Qual seria o intervalo
que em $99\%$ das vezes iria conter o verdadeiro valor predito
considerando a nova informação de $x_1$?

Calcularemos o intervalo de predição com $99\%$ de confiança para a nossa amostra de teste.


***Intervalos de predição***

*Lembrando:*

$Y_0$_est $= \beta_0$_est $+ \beta_1$_est$\ *\ x_1$_novo.

$Y_0$ e $Y_0$_est são independentes.

$t_1 =$ quantil da dist. $t(\alpha/2,n-2)$.

$t_2 =$ quantil da dist. $t(1-\alpha/2,n-2)$.

$\alpha = 0,05$.


```{r exem1}
x1_novo <- teste$x1  
#x1_novo <- c(12,20,48,51,57,62)
Y0_est <- b0_est + b1_est*x1_novo
auxiliar_y0 <- sqrt(QMres*(1+ 1/n + (x1_novo - mean(x1)) /Sxx ))
```

```{r exey}
Y0_est_min <- Y0_est - t2*auxiliar_y0
Y0_est_max <- Y0_est - t1*auxiliar_y0
```

```{r ixexem}
IC_Y0_est <- tibble(Y0_est_min, Y0_est_max, y = teste$y)
IC_Y0_est %>% kable(caption = "Intervalo de predição para amostra de teste")
```
Como podemos ver, o valor de $y$ real estava dentro do intervalo para as quatro observações.


## ***Análise de Variâcia***

A Análise de Variâcia com todos os valores (Graus de Liberdade, SQTotal, SQRes, SQReg, QMRes, QMReg e F).

***ANOVA***

Vamos testar a significancia da regressão através da Análise de Variância (ANOVA), nesse
 caso testariamos se $\beta_1 = 0$.

**Soma do quadrado da Regressão**
```{r saqreganova}
SQreg <- b1_est*Sxy
SQreg
```

**Soma do quadrado total**
```{r asqtotalanova}
SQtotal <- sum((y-mean(y))^2)
SQtotal
```

**Soma do quadrado do resíduo**
```{r asqresanova}
SQres <- sum((y-mean(y))^2) - b1_est*Sxy
SQres
```

```{r sqresanova}
QMreg <- SQreg
QMreg
```

**Lembre-se que QMres eh o estimador de sigma^2 e QMres=SQres/(n-2)**
```{r faanova}
F_0 <- QMreg/QMres
F_0
```

**Quantil da Distribuição F-Snedecor**
```{r f1aanova}
f1 <- pf(F_0, df1 = 1, df2 = n-2, lower.tail = F)
f1
```

```{r descisaaoanova}
if(F_0 > f1){
  cat("Rejeita-se H0")
}
```

***Usando funções do R***

```{r}
fit <- lm(y~x1)
summary(fit)
```
Podemos ver que obtivemos coeficientes similares em nosso modelo.

```{r}
anova(fit)
```
Podemos notar que todos os resultados são similares ao já obtidos.

***Normalidade dos resíduos***
```{r normalidadtestresd}

Anovamodel <- aov(y ~ x1, data = dados)

shapiro.test(resid(Anovamodel))
```

A hipótese nula do Teste de Shapiro-Wilk é de que não há diferença entre a nossa distribuição dos dados e a distribuição normal. O valor-p maior do que 0.01 nos dá uma confiança estatística para afirmar que as distribuição dos nossos resíduos não difere da distribuição normal.

<<<<<<< HEAD
Dessa forma nossos dados satisfazem todas as premissas da ANOVA e portanto, o resultado da nossa ANOVA são válidos, confirmando nossa premissa de que os resíduos possuem distribuição normal.
=======
Dessa forma nossos dados satisfazem todas as premissas da ANOVA e portanto, o resultado da nossa ANOVA são válidos.
>>>>>>> c5734a8498079e1da24ef9ce5195c70e77b5478d
