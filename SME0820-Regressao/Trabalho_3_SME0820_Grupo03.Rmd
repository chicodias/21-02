---
title: "SME0820 - Modelos de Regressão e Aprendizado Supervisionado I"
subtitle: "Trabalho 3 - Grupo 3"
author:
  - Brenda da Silva Muniz 11811603
  - Francisco Rosa Dias de Miranda 4402962
  - Heitor Carvalho Pinheiro 11833351
  - Mônica Amaral Novelli 11810453
date: "Dezembro 2021"
output: pdf_document
---


\centering
\raggedright
\newpage
\tableofcontents
\newpage

## Objetivo

Este trabalho tem como objetivo ajustar um modelo de regressão linear múltipla a um conjunto de dados.

## Conjunto de dados

O dataset contém dados de um experimento para determinar **pressão**, **temperatura**, **fluxo de CO2**, **umidade** e **tamanho da partícula de amendoim** sob o **rendimento total de aceite por lote de amendoim**. [rendimento (y)].

Iremos trabalhar com uma significância de 97%. 

Podemos, abaixo, visualizar as dez primeiras observações do conjunto de dados:

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
library(ggpubr)
library(gridExtra)
library(GGally)
library(pander)
library(car)
library(rstatix)
library(lmtest)
library(QuantPsyc)
library(psych)
library(corrplot)
```

```{r, echo=TRUE, warning=FALSE}
dados <- read_csv("dados/data-table-B7.csv", locale = locale(decimal_mark = ","))
n <- length(dados$y)

# Renomeando as colunas
names(dados) <- c("Pressao", "Temp", "FluxoCO2", "Umidade", "Tamanho", "y")

head(dados)
```


## 1. Análise Descritiva dos dados

Temos cinco covariáveis quantitativas e a coluna $y$ corresponde a nossa variável preditora que determina **o rendimento total de aceite por lote de amendoim**. Renomeando nossas covariáveis para facilitar a manipulação destas, temos:

- $Y:$ Rendimento total de aceite por lote de amendoim*.

- $X_1:$ Pressão

- $X_2:$ Temperatura

- $X_3:$ Fluxo de CO2

- $X_4:$ Umidade

- $X_5:$ Tamanho

Realizando a concatenação das mesmas em um data frame:

```{r}
x1 <- dados$Pressao
x2 <- dados$Temp
x3 <- dados$FluxoCO2
x4 <- dados$Umidade
x5 <- dados$Tamanho
y <- dados$y

data <- data.frame(cbind(x1, x2, x3, x4, x5, y))
```

Para obsermos um resumo dos dados, podemos utilizar as funções _glimpse_ e _summary_, que nos retornarão, respectivamente, os tipos de variáveis e o máximo de observações possíveis no espaço proposto na horizontal; e, medidas descritivas das nossas variáveis, sendo estas os valores mínimo e máximo das observações, o primeiro quantil, a mediana, a média e o terceiro quantil.

```{r}
glimpse(data) 
```

```{r}
summary(data) 
```

Salvando o número total de observações em uma variável:

```{r}
n <- length(dados$y)
n
```

### Gráficos de barras

```{r out.height="70%"}
dados %>%
  pivot_longer(cols = everything()) %>%
  ggplot() +
  geom_bar(aes(x = as_factor(value)), stat = "count") +
  facet_wrap(~name, scales = "free_x") +
  labs(
    x = "Variáveis",
    y = "Valores",
    title = "Gráfico de Barras - Conjunto de Dados"
  ) +
  theme_minimal()
```

A partir dos gráficos de barras, podemos ver que nossas cinco covariáveis, apesar de serem quantitativas, assumem apenas dois valores, com a mesma proporção. A única variável que assume mais valores do que isso é $y$, que apararenta ter uma distribuição quase uniforme.

Outros gráficos que comparam as relações entre nossas variáveis é o gráfico de coordenadas paralelas e nossa matriz de correlação, ambos também explicitando a falta de correlação entre as covariáveis.

### Correlações

Visualizando a correlação de Pearson entre as covariáveis e $y$*  por meio de uma tabela:

```{r}
C_tabela <- cor(data)
C_tabela
```

Fazendo uso destas correlações dispostas, podemos construir uma matriz de gráficos para expor as relações entre as variáveis. Com isso, poderemos visualizar as densidades de frequência na diagonal desta matriz, gráficos de dispersão no painel triangular inferior e  coeficientes das correlações no superior. Também faremos outro tipo de gráfico de correlação para auxiliar esse processo: um mapa de calor com visualização gráfica por meio de círculos.

```{r message = F}
ggpairs(data, lower = list(continuous = "smooth")) + ggtitle("Gráfico de pares - Dados")
```
```{r}
corrplot(C_tabela, method = "circle")
```
Observando o comportamento da correlação das nossas variávies e analisando os resultados dispostos, vemos que nenhuma das covariáveis se correlacionam entre si. Além disso, a maioria apresenta uma correlação muito baixa com a variável preditora - com exceção de x5 (Tamanho) - que apresenta uma correlação alta -, e x2 - que, apesar de apresentar uma correlação relativamente baixa, se torna significante devido ao cenário obtido.

Essa ausência de correlação pode ser explicada pelo comportamento em "X" da maior parte das covariáveis, que também pode ser notado através do gráfico de coordenadas paralelas:


```{r  out.height="70%"}
ggparcoord(dados) + labs(
  x = "Variáveis",
  y = "Valores",
  title = "Coordenadas Paralelas - Dados"
) +
  theme_minimal()
```


Definindo a covariável Tamanho como mapeamento para cor, podemos dispor outra versão dos gráficos de pares:

```{r  out.height="70%"}
dados %>%
  pivot_longer(!c(Tamanho, y)) %>%
  ggplot(aes(y = y, color = as_factor(Tamanho))) +
  geom_point(aes(x = value)) +
  facet_wrap(~name, scales = "free_x") +
  labs(
    x = "Variáveis",
    y = "Valores",
    title = "Gráficos de dispersão - Dados",
    color = "Tamanho"
  ) +
  theme_minimal()
```

Note como a covariável Tamanho foi capaz de separar bem as variáveis no eixo y, enquanto que o mesmo feito não foi alcançado no eixo x. Temos aqui fortes indícios de independência entre as covariáveis, e o melhor modelo talvez não seja o que contenha todas elas, como veremos mais adiante.

## 2. Matriz Hat

Criando e visualizando a nossa matriz X, em que a primeira coluna corresponde a uma repetição de números 1, a segunda na covariável x1 e, a terceira, x2:

```{r}
X <- matrix(c(rep(1,n), x1, x2), ncol = 3, nrow = n, byrow = FALSE)

X
```
Definindo e visualizando nossa matriz Y, que contém apenas uma coluna e é a da variável preditora y - o vetor resposta:

```{r}
Y <- matrix(dados$y, ncol = 1, nrow = length(dados$y))

Y
```

A partir disso, podemos construir nossa matriz HAT, em que:

$Hat: H = X(X^TX)^{−1}X^T$, onde
$h_{ii}$: i-esimo elemento da diagonal de H;
$h_{ij}$: elemento ij da matriz H.

```{r}
H <- X %*% solve(t(X) %*% X) %*% t(X)
h <- diag(H)
summary(h)
```

```{r}
dadoshii <- data.frame(cbind(x1, x2, h))

View(dadoshii)

```

Temos que:

$h_{ii} = \frac{1}{n} +\frac{(X_i − \overline{X})^2}{S_{XX}}$

Levando isso em consideração, $h_{ii}$ deve atingir seu menor valor no ponto $\overline{X}$, se igualando a $\frac{1}{n}$. 
Temos então que $h_{ii}$ é uma medida de alavanca, que nos informa o quão distante do centro a observação está. Ou seja, quanto mais a observação se distancia de $\overline{X}$, mais $h_{ii}$ cresce. Com isso, é possível determinarmos possível outliers - uma vez que, se $h_{ii}$ for relativamente maior do que os das demais observações, temos que ele provavelmente será um ponto influente e distante dos demais (e da média).

Analisando os resultados obtidos na tabela acima, e aplicando esses pontos, podemos observar que todos nossos pontos possuem a mesma influência, uma vez que $h_{ii}$ se mantém constante para todas as observações. Logo, provavelmente também não contamos com outliers dentre as nossas observações.

Calculando $\frac{1}{n}$ e o comparando com nosso $h_{ii}$:

```{r}

frac_n = 1/n

frac_n

```

Como contamos com  $h_{ii} = 0.1875$ para todos os valores da tabela, temos que os pontos se encontram com um afastamente uniforme dentre eles da média. 

## 3. Análise de resíduos

Primeiramente, vamos construir nosso modelo e utilizar a função _summary_ para observarmos as principais medidas descritivas do mesmo, sendo:

```{r}
mod <- lm(y ~ x1 + x2, dados) 
```

```{r}
summary(mod)
```

```{r}
par(mfrow=c(2,2)) 
```

```{r}
plot(mod) 
```

1) Para a primeira plotagem, obtemos o gráfico dos resíduos comparados com os valores ajustados, onde é possível avaliar o pressuposto de linearidade. 
Nesse gráfico, podemos notar que a linha vermelha está muito próxima de estar completamente no eixo horizontal, uma vez que o balanceamento de valores é muito equilibrado. Ou seja, não temos nenhuma observação que influencia nosso ajuste muito fortemente - seja positivamente ou negativamente.

2) Já no segundo gráfico temos um QQ plot. Nele, podemos verificar se os resíduos apresentaram distribuição normal.  
Podemos ver que, existe sim uma tendência à esta distribuição - principalmente nos pontos  mais centrais; entretanto, há um leve afastamento nas extremidades, e um falta de preenchimento de espaço no centro do gráfico, o que pode nos indicar que essa tendência não é tão forte.

3) No terceira gráfico era esperado termos os resíduos estandardizados vs valores ajustados, que serviria para verificar a  homocedasticidade. Entretanto, por conta dos valores da nossa matriz HAT serem uniformes, não é possível realizar tal plotagem. 

4) Na última plotagem, podemos verificar caso existam dutliers e possíveis pontos influentes, reiterando o pressuposto no item 2 do trabalho. Como podemos ver, há uma influência alternada entre positiva e negativa, porém em aproximadamente mesmos graus de intensidade e distanciamento. Também podemos verificar a ausência de outliers, uma vez que, se houvesse, deveria haver uma linha pontilhada vermelha com pontos para fora desta. 



## 4. Testes nos resíduos

  - resumo dos residuos
  - análise dos testes de normalidade
  - teste de homocedasticidade
  - teste de multicolinearidade

```{r}
summary(res)
```
```{r}
#Análise gráfica
## Análise gráfica:
par(mfrow=c(2,2))

plot(fit)
```


O resumo dos resíduos nos indica que provavelmente devem existir outliers, como o valor $12.625$, que se afasta muito da mediana e do 3º quantil.

Vale notar que o gráfico dos resíduos studentizados x valores ajustados indiac que não há homocesdaticidade nos dados, uma vez que a linha vermelha apresenta um padrão retangular. Iremos confirmar essa hipótese com o teste de Breusch Pagan em breve.

```{r}
pander(shapiro.test(res),
  style = "rmarkdown",
  caption = "Teste de normalidade Shapiro-Wilk para os resíduos"
)
```


O teste acima confirma nossa suposição de que os resíduos têm distribuição Normal, pois, para um nível de significância de 97%, o valor-p obtido, 0,7669, não rejeita a hipótese nula, de normalidade dos dados.

**Teste de Homoscedasticidade**

```{r}
#Teste de homoscedasticidade
bptest(fit)
```

Dado que o p-valor para o teste de Breusch-Pagan é menor que 0.03, pode-se concluir que há heterocedasticidade nos dados, portanto rejeitamos a hipótese nula, $H_0$.

**Teste de Multicolinearidade**

Já verificamos anteriormente através dos gráficos de dispersão que não existe colinearidade entre a maioria das variáveis independentes. Podemos formalizar este resultado através da medida $VIF$

```{r}
## Ausência de Multicolinearidade

vif(fit)
```
Considerando o nosso modelo reduzido, uma vez quE $VIF = 1$ para as duas covariáveis, podemos concluir que não existe correlação alguma entre elas.

**ANOVA Teste**

Além disso, para determinar matematicamente se existe uma relação linear entre a variável resposta $\boldsymbol{Y}$ e qualquer as outras covariáveis $\boldsymbol{X}_1,\ldots,\boldsymbol{X}_k$, é possível utilizar o teste **ANOVA**. Nele, queremos testar:

**$H_0$**: Nenhuma das variáveis contribui significativamente ao modelo, versus:

**$H_a$**: Pelo menos uma das covariáveis contribui significativamente ao modelo.

```{r}
anova(fit)
```



## 5. Resíduos Escalonados


Iniciamos essa sessão apresentando os coeficientes ajustados de nosso modelo.

```{r}
(betas <- as.vector(fit$coefficients))
```

$$Y = 80.134 +0.282 x_2 -16.065x_5$$

### Interpretação dos coeficientes:

 - $\beta_0$: Quando todos os $x_i$ são iguais a zero, o valor esperado de $y$ é de 80,134.
 - $\beta_2$: Em média, para cada aumento de 1 ponto na Temperatura, esperamos um aumento de 0,282 em $y$, com todo o resto mantido constante.
 - $\beta_5$:  Em média, a cada aumento de 1 ponto no Tamanho, é esperado um descréscimo de 16,065 unidades em $y$, com todo o resto mantido constante.
 
 

É útil trabalharmos com o escalonamento dos resíduos para encontrarmos **outliers**, observações que estejam de alguma maneira separadas do resto dos dados.

### Quadrado Médio dos Resíduos

```{r}
p <- 3 # número de parâmetros estimados

QMRes <- sum(res^2) / (n-p)
cat("QMres: ", QMRes)
```
 



  - estimativas do modelo (betas)
  - residuos e QMres
  - residuos padronizados
  - residuos studentizados internamente
  - residuos studentizados externamente
  - observações que possam ser remotas no espaço
  - histograma delas e analisar


### Resíduo Padronizado

Sendo a variância média dos resíduos estimada por $QM_{res}$, para torna-lá unitária basta fazermos:

$$d_i = \frac{e_i}{\sqrt{QM_{res}}},\ \  i = 1, ..., n.$$

Consequentemente, valores grandes (como, digamos, $d_i > 3$) potencialmente indica um **outlier**.

```{r}
res.padr <- res / sqrt( QMRes)
res.padr
```

Note que $QM_{res}$ é apenas uma aproximação para a variância do i-ésimo resíduo, o que pode ocasionar em distorções em sua estimação.

### Resíduo Studentizado Internamente

Podemos refinar o método anterior escalonando o resíduo pelo desvio-padrão 'exato' do i-ésimo resíduo e levando em consideração onde o ponto da variável está no espaço.

Utilizando a matriz hat, podemos estimar a variância do i-ésimo resíduo como sendo:

$$Var(e_i) = \sigma^2(1-h_{ii})$$

Onde $h_{ii}$ é o i-ésimo elemento da diagonal da matriz Hat. Ainda mais, como essa é uma medida de **locação** do i-ésimo ponto com respeito a $x$, a variância de $e_i$ depende de onde o ponto $x_i$ está.

```{r}
#  +++ pto => +hii => 1-hii -- => +++ res.int.st  
res.int.st <- res / sqrt( QMRes * (1 - h))
res.int.st
```

Não se observou nenhum outlier segundo o critério discutido acima.

```{r}
# CURIOSIDADE
#obs
p/n
```

### Resíduo Studentizado Externamente

Primeiro calculamos o **QMRes** do resíduo sem a $i$-ésima observação, com $i = 1,\ldots, n$ (cálculo das $n$ variâncias sem a $i$-ésima observação, com $i = 1,\ldots, n$).

```{r}
S_i <- ( (n - p) * QMRes - res^2 / (1 - h)  ) / (n - p - 1)
S_i
```

Se não tivermos nem uma observação influente, esperamos que res.int.st esteja próximo de res.ext.st. Se tivermos a $i$-ésima observação influente então esperamos que o i-ésimo res.ext.st seja maior em comparação com o $i$-ésimo res.int.st.


```{r}
res.ext.st <- res / sqrt( S_i * (1 - h))
res.ext.st
```

Vamos observar se há observações que podem ser remotas no espaço,

```{r}
sort(S_i)
hist(S_i) 
```






## 6. Comparações resíduos escalonados


```{r}
nome_colunas <- c("i", "e_i", "d_i", "r_i", "h_ij", "t_i")
tab <- tibble(i= 1:16,res, res.ext.st, res.int.st,h,res.padr)
kable(tab, col.names = nome_colunas, format = "markdown")
```


  - quadro comparativo com os resultados obtidos no item anterior
  - análise de cada um dos resíduos calculados (aula 17)
  
  
## 7. Gráfico de Resíduos versus ajuste

  - análise do gráfico para cada um dos residuos calculados no item 5 vs valores ajustados
  
```{r}
  tab %>% dplyr::select(!c(i,h)) %>%
  mutate(y = dados$y) %>% 
  pivot_longer(!y) %>% 
  ggplot() +
  geom_point(aes(x = y, y = value)) +
  geom_hline(yintercept = 0) +
  facet_wrap(~name)
```

Não foi observado nenhum padrão nos plots residuais, o que nos fornece indícios gráficos da aleatoriedade dos resíduos.
  
  
## 8. Transformações

  - proponha uma transformação para seu modelo que corrija possíveis problemas e compare
  - refazer os itens de 1 a 7 com o novo modelo

## 9. Teste de Falta de ajuste

  - proponha um caso ou exemplo onde seja necessario a aplicação do teste da falta de ajuste
  - residuos vs valores ajustados
  - mostre a falta de ajuste dos dados
  - ajuste o modelo
  - ANOVA
  - SQEP
  - SQFA
  - análise do teste F_0
  
**Criação de dataset artificial**

Resolvemos utilizar um dataset que contém dados do comprimento da mandíbula de veados com relação à idade do animal.

```{r}
dados2 <- read_delim("jaws.txt", "\t")
#Ajustando um modelo linear
fit2 <- lm(bone ~ age, data = dados2)
summary(fit2)
```


**Gráfico dos resíduos vs Valores Ajustados**



  
## 10. Mínimos Quadrados Ponderados

  - proponha um caso ou exemplo onde seja necessário a aplicação da técnica dos mínimos quadrados e faça a respectiva análise

A Continuação será apresentado um exemplo simulado de falta de ajuste.

Exemplo: Um pesquisador no setor de vendas queria estudar a associação entre o faturamento mensal médio de vendas de lanches (Y) e a despesa por mês com divulção(X). Os dados referentes a 30 lanchonetes encontram-se abaixo:

```{r}
x<-c(2,2,2,4,4.0,4,8,8,8,8,8,8,11,11,11,11,11,14,14,14,14,14,14,16,16,16,18,18,18,18)
y<-c(89,73,72,80,112,99,79,114,116,92,131,149,109,157,103,147,82,113,149,121,99,103,110,170,189,122,203,115,217,204)
gasto_venda= data.frame(cbind(x,y))
```

Note que que estes dados indicam falta de ajuste a um modelo linear

```{r}
pairs.panels(gasto_venda)
```

Portanto, devemos realizar o ajuste do modelo utilizando o Método de Mínimos Quadrados Ponderados. Para isso deve-se observar as estimativas do Erro Puro para cada nível de X, ou seja, os valores de Var(Y | X). Observe a função abaixo:

```{r}
 tapply(gasto_venda[,2],as.factor(gasto_venda[,1]),var)
```
```{r}
gasto <-c(2,4,8,11,14,16,18)
v<-c(91.0000,  259.0000,  645.1000,  987.8000,  323.3667, 1192.3333, 2202.9167)
plot (gasto,v,xlab="Gasto",ylab="Variância (Venda|Gasto)" )
```

Observa-se que Var(Venda | Gasto) é proporcional ao Gasto. Sendo assim, o peso $W_i$ deve ser
inversamente proporcional ao $X_i$.

```{r}
wi <- c(1/2 ,1/4 ,1/8 ,1/11 ,1/14 ,1/16 ,1/18)
valores_peso= data.frame(cbind(gasto,wi))
valores_peso %>% 
  knitr::kable()
```

Abaixo encontra-se nosso comando R para o ajuste do modelo via Método de Mínimos Quadrados Ponderados e a respectiva saída do software com os coeficientes ajustados.

### Cálculo do ajuste_ponderado 

```{r}
ajuste_ponderado=lm(formula = y ~ x, weights = 1/x)
summary(ajuste_ponderado)
```

### Cálculo da Anova

```{r}
anova(ajuste_ponderado)
```

### Gráficos para Análise dos Resíduos

```{r}
plot(x,ajuste_ponderado$residuals,
     main = expression(paste("Resíduos vs Gasto")),
     xlab="Valores Ajustados Ponderados",ylab="Resíduos")
```

```{r}
plot(ajuste_ponderado$fitted.values,ajuste_ponderado$residuals,
     main = expression(paste(" Resíduos vs Valores ajustados")),
     xlab="Valores Ajustados Ponderados",ylab="Resíduos")
```

As Figuras acima evidenciam que o problema da heterocedasticidade dos erros foi solucionado, pois nos dois
gráficos os resíduos ponderados estão dispostos homogeneamente em torno de zero.

Note também, que os coeficientes (Betas) estimados seriam:

```{r}
b0_est=ajuste_ponderado$coefficients[1]
b1_est=ajuste_ponderado$coefficients[2]
cbind(b0_est,b1_est)
```

### Gráfico da reta ajustada aos dados
```{r}
plot(x,y,
 main = expression(paste("Reta ajustada com ",
 hat(beta)[0],"=70.03587",
 " e ", hat(beta)[1],"=4.978227")),
 xlab = "x", ylab = "y")
curve(b0_est + b1_est*x, add = T, col = 'red')
```  
