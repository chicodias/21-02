---
title: "Lista3 - Grupo3"
author: 
  - Brenda da Silva Muniz 11811603
  - Francisco Rosa Dias de Miranda 4402962
  - Heitor Carvalho Pinheiro 11833351
  - Mônica Amaral Novelli 11810453
date: "23/11/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tidyverse)
library(ggpubr)
library(FSA)
```

6) Considere o teste da falta de ajuste para o modelo de regressão linear simples. Obtenha E(QMres). Desenvolva passo a passo colocando todos os detalhes.

O teste da falta de ajuste consiste em dividir a soma de quadrados do resíduo ($SS_{res}$) em duas partes: soma de quadrados devido ao erro puro (denotaremos como $SS_{EP}$) e a soma de quadrado devido a falta de ajuste (denotaremos como $SS_{FA}$).

Podemos escrever isso como:

$SS_{res} = SS_{EP} + SS_{FA}$

Como visto na aula 18, temos que para cada $X_i = 1,...,m$, a contribuição para a soma do quadrado do erro puro é a soma do quadrado interno de $Y_{ij}$ em relação a $\overline{Y_{i}}$, $\sum_{j=1}^{n_i}(Y_{ij}-\overline{Y_{i}})^2$, onde $\overline{Y_{i}} = \sum_{j=1}^{n_i}Y_{ij}$ e, portanto, $SQ_{EP} = \sum_{i=1}^{m} \sum_{j=1}^{ni}(Y_{ij} - \overline{Y_{i}})^2$ (independente do modelo a ser ajustado), com:

$\sum_{i=1}^{m} (n_i - 1) g.l. = (\sum_{i=1}^{m}ni - \sum_{i=1}^{m}1)g.l. = (n-m)g.l.$

Introduzido tais dados, podemos desenvolver $SS_{res} = SS_{EP} + SS_{FA}$.

Nota-se que $Y_{ij} - \hat{Y}_i = (Y_{ij} - \overline{Y_i})+(\overline{Y_i} - \hat{Y_i})$. Elevando ambos os lados ao quadrado e realizando o somatório para i e j, fazemos:

$(Y_{ij} - \hat{Y}_i)^2 = (Y_{ij} - \overline{Y_i})^2 + (\overline{Y_i} - \hat{Y_i})^2 + 2(Y_{ij} - \overline{Y_i})(\overline{Y_i} - \hat{Y_i})$

$\sum_{i=1}^{m} \sum_{j=1}^{n_i}(Y_{ij} - \hat{Y}_i)^2 = \sum_{i=1}^{m} \sum_{j=1}^{n_i}(Y_{ij} - \overline{Y_i})^2 + \sum_{i=1}^{m} \sum_{j=1}^{n_i}(\overline{Y_i} - \hat{Y_i})^2 + 2\sum_{i=1}^{m} \sum_{j=1}^{n_i}(Y_{ij} - \overline{Y_i})(\overline{Y_i} - \hat{Y_i})$

Desenvolvendo apenas $\sum_{i=1}^{m} \sum_{j=1}^{n_i}(Y_{ij} - \overline{Y_i})(\overline{Y_i} - \hat{Y_i})$, temos:

$\sum_{i=1}^{m} \sum_{j=1}^{n_i}(Y_{ij} - \overline{Y_i})(\overline{Y_i} - \hat{Y_i}) = \sum_{i=1}^{m}(\overline{Y_i} - \hat{Y_i}) \sum_{j=1}^{n_i}(Y_{ij} - \overline{Y_i}) = \sum_{i=1}^{m}(\overline{Y_i} - \hat{Y_i})(\sum_{j=1}^{n_i} Y_{ij} - n_i\overline{Y_i}) = \sum_{i=1}^{m}(\overline{Y_i} - \hat{Y_i})(n_i\overline{Y_i} - n_i\overline{Y_i})  = \sum_{i=1}^{m}(\overline{Y_i} - \hat{Y_i})(0) = 0 $

Logo, temos que:

$\sum_{i=1}^{m} \sum_{j=1}^{n_i}(Y_{ij} - \hat{Y}_i)^2 = \sum_{i=1}^{m} \sum_{j=1}^{n_i}(Y_{ij} - \overline{Y_i})^2 + \sum_{i=1}^{m} \sum_{j=1}^{n_i}(\overline{Y_i} - \hat{Y_i})^2 + 2 * 0 = \sum_{i=1}^{m} \sum_{j=1}^{n_i}(Y_{ij} - \overline{Y_i})^2 + \sum_{i=1}^{m} \sum_{j=1}^{n_i}(\overline{Y_i} - \hat{Y_i})^2$

A soma de quadrados devido ao erro puro é obtida por: 

$SS_{EP} = \sum_{i=1}^{m} \sum_{j=1}^{n_i}(Y_{ij} - \overline{Y_i})^2 $

e, a Soma de quadrado devido a falta de ajuste, por:

$SS_{FA} = \sum_{i=1}^{m} n_i (\overline{Y_i} - \hat{Y_i})^2 $





7) Uma empresa fabricante de garrafas de vidro registrou dados sobre a quantidade média de
defeitos por 10 000 garrafas, devido a pedras (pequenas pedras embutidas na parede da garrafa)
e a quantidade de semanas a partir da última reparação geral feita no forno.

```{r dados07}
#definindo os dados
defeitos <- c(13.00, 16.10, 14.50, 17.80, 22.00, 27.40, 16.80, 34.20, 65.60, 49.20, 66.20,81.20,87.40, 114.50)

semanas <- c(seq(4,17))

df <- data.frame(defeitos, semanas)

View(df)
```
a)
## Ajustando um modelo de regressão linear MMQ

```{r}
fit1 <- lm(defeitos ~ semanas, data = df)
summary(fit1)
```
## Plotando o modelo ajustado

```{r}
ggplot(df, aes(x=semanas, y=defeitos)) +
  geom_point() +
  stat_smooth(method = "lm", col = "red") +
  theme_pubclean()
  
```
Percebemos que os atributos *semanas* e *defeitos* apresentam certa linearidade, com um $R^2_{aj} = 0.84$. Porém, precisamos verificar os resíduos para determinar a adequabilidade do modelo.

### Gráfico de Resíduos

```{r}
FSA::residPlot(fit1)
```
O gráfico de resíduos acima nos indica que os resíduos não são lineares, apesar da distribuição deles parecer ser Normal, de acordo com o histograma ao lado.

```{r}
ggplot(df, aes(x = fit1$fitted.values, y = fit1$residuals)) +
  geom_point() + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth()
```


Isso significa que ainda existe alguma relação entre as variáveis que o modelo de regressão linear simples não é capaz de detectar.

Devido a isso, precisamos realizar algumas transformações no nosso modelo.

b) 

Uma vez que os resíduos apresentam um comportamento semelhante a uma função quadrática, ajustaremos uma regressão polinomial de grau 2.

## Ajustando um modelo de regressão polinomial

```{r}
fit2 <- lm(defeitos ~ poly(semanas,2), data = df)
summary(fit2)
```
Percebemos que o modelo polinomial é muito melhor para explicar a variabilidade nos dados, alcançando um $R^2_{aj} = 0.94$.

```{r}
#usando o ggplot
ggplot(df, aes(x=semanas, y=defeitos)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ poly(x,2), col = "red") +
  theme_pubclean()
```
## Gráfico dos resíduos para o modelo polinomial

```{r}
ggplot(df, aes(x = fit2$fitted.values, y = fit2$residuals)) +
  geom_point() + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth()
```
O modelo polinomial apresenta resíduos bem menos correlacionados que o modelo mínimos quadrados, o que garante uma melhor adequalibilidade.
